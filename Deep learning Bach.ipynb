{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Bach. A step-by-step guide.\n",
    "\n",
    "I decided to merge two of my great passions: for Artificial Intelligence and for Bach music and try to guide You step-by-step through creating a computer system, which by examining Bach's collected works learns itself to compose \"like Bach\"... Seriously, at the end of this tutorial you will have a working artificial composer and hopefuly you will also understand how and why it works. And I am not going to lie to you: similar systems of course already exist, created by very smart university and corporate reserchers, but also by hobbyists.\n",
    "\n",
    "We live in exciting times. Amount of data available on-line in public domain is incredible and tools that allow to manipulate that data in really interesting ways (read: Machine Learning) matured. One thing in all of that is really to be thankful for: somehow the idea of sharing prevails and lot of extremely valuable stuff just lies there, waiting to be used and very smart people spend incredible amounts of their time making even more stuff publicly available and understandable. Kudos. I owe them. Hence this guide.\n",
    "\n",
    "Callout: Why Bach?\n",
    "\n",
    "Callout: What is a Machine Learning / Deep Learning\n",
    "\n",
    "So, who is the audience of this guide?\n",
    "\n",
    "Cerainly you are not an expert in deep learning nor in musicology. You \n",
    "\n",
    "So, what are we going to do?\n",
    "\n",
    "Lets try to sketch\n",
    "\n",
    "## Plan of attack\n",
    "\n",
    "As always divide and concquer is highly succesful strategy, so let's try:\n",
    "\n",
    "1. We will start by creating a workbench we're going to use.\n",
    "2. Then we need to get a lot of example data to teach our composer\n",
    "3. Understand the data enough to make it useful\n",
    "4. Prepare it so it's suitable for ML\n",
    "5. Build a neural network\n",
    "6. And teach it using the data we prepared\n",
    "7. Paradoxically, this is not the end. We now need to uncover all that innate knowledge and make it express itself in writing\n",
    "8. Now, let's try to ask it to compose something for us\n",
    "9. Turn that partiture into playable MIDI file and...\n",
    "10. Finally, play it!\n",
    "\n",
    "Callout: Useful links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "from os.path import basename\n",
    "from itertools import permutations\n",
    "\n",
    "vocabulary_size = 30**4\n",
    "MAX_VOICES = 4\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "def permutate(s, pat):\n",
    "    assert len(s) == len(pat), \"string len: {} != pattern len: {}\".format(len(s), len(pat))\n",
    "    r = []\n",
    "    for ind in pat:\n",
    "        r.append(s[ind])\n",
    "    return \"\".join(r)\n",
    "\n",
    "def read_data_files(path, validation=True):\n",
    "    \"\"\"Read data files according to the specified glob pattern\n",
    "    Optionnaly set aside the last file as validation data.\n",
    "    No validation data is returned if there are 5 files or less.\n",
    "    :param directory: for example \"data/*.txt\"\n",
    "    :param validation: if True (default), sets the last file aside as validation data\n",
    "    :return: training data, validation data, list of loaded file names with ranges\n",
    "     If validation is\n",
    "    \"\"\"\n",
    "    codetext = []\n",
    "    opusranges = []\n",
    "    bachlist = glob.glob(path + '/**/*.txt', recursive=True)\n",
    "    for bachfile in bachlist:\n",
    "        bachtext = open(bachfile, \"r\", encoding='utf8')\n",
    "        start = len(codetext)\n",
    "        bars = (bachtext.read()).split(\"!\")\n",
    "        bars2 = []\n",
    "        nb_voices = len(bars[0])\n",
    "        if nb_voices<=MAX_VOICES:\n",
    "            print(\"Loading file: {} ; {} voices\".format(bachfile, nb_voices))\n",
    "            for bar in bars: \n",
    "                bar2 = bar.ljust(MAX_VOICES, \" \")\n",
    "                #print(\"'{}', '{}'\".format(bar, bar2))\n",
    "                bars2.append(bar2)\n",
    "            #print(\"bars2:\", bars2)\n",
    "            codetext.extend(bars2)\n",
    "            end = len(codetext)\n",
    "            opusranges.append({\"start\": start, \"end\": end, \"name\": basename(bachfile).split(\".\")[0]})\n",
    "            bachtext.close()\n",
    "\n",
    "            patterns = list(permutations(range(MAX_VOICES)))\n",
    "\n",
    "            for pattern_no in range(1,len(patterns)):\n",
    "                start2 = len(codetext)\n",
    "                bars = []\n",
    "                for j in range(start, end-1): #iterate over the whole opus except the final divider (\"********\")\n",
    "                    #print(codetext[j], patterns[pattern_no])\n",
    "                    bars.append(permutate(codetext[j],patterns[pattern_no]))\n",
    "                assert codetext[end-1] == \"********\", \"expecting '********', instead '{}'\".format(codetext[end])\n",
    "                bars.append(codetext[end-1]) \n",
    "                codetext.extend(bars)\n",
    "                end2 = len(codetext)\n",
    "                opusranges.append({\"start\": start2, \"end\": end2, \"name\": \"Perm\"+str(pattern_no)+basename(bachfile).split(\".\")[0]})\n",
    "        else:\n",
    "            print(\"Skipping file: {} ; {} voices\".format(bachfile, nb_voices))\n",
    "    if len(opusranges) == 0:\n",
    "        sys.exit(\"No training data has been found. Aborting.\")\n",
    "    \n",
    "    total_len = len(codetext)\n",
    "    \n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(codetext)\n",
    "    \n",
    "    # For validation, use roughly 90K of text,\n",
    "    # but no more than 10% of the entire text\n",
    "    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "    # 10% of the text is how many files ?\n",
    "    validation_len = 0\n",
    "    nb_opus1 = 0\n",
    "    for opus in reversed(opusranges):\n",
    "        validation_len += opus[\"end\"]-opus[\"start\"]\n",
    "        nb_opus1 += 1\n",
    "        if validation_len > total_len // 10:\n",
    "            break\n",
    "\n",
    "    # 90K of text is how many books ?\n",
    "    validation_len = 0\n",
    "    nb_opus2 = 0\n",
    "    for opus in reversed(opusranges):\n",
    "        validation_len += opus[\"end\"]-opus[\"start\"]\n",
    "        nb_opus2 += 1\n",
    "        if validation_len > 90*1024:\n",
    "            break\n",
    "\n",
    "    # 20% of the books is how many books ?\n",
    "    nb_opus3 = len(opusranges) // 5\n",
    "\n",
    "    # pick the smallest\n",
    "    nb_opus = min(nb_opus1, nb_opus2, nb_opus3)\n",
    "\n",
    "    if nb_opus == 0 or not validation:\n",
    "        cutoff = total_len\n",
    "    else:\n",
    "        cutoff = opusranges[-nb_opus][\"start\"]\n",
    "    validata = data[cutoff:]\n",
    "    codedata = data[:cutoff]\n",
    "    return data, codedata, validata, opusranges, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Zufek/ml/music_rnn'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file: ../../ml/music_rnn/txt/major/9/bjsbmm12.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/9/bjsbmm07.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/9/bjsbmm14.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/9/bwv29sin.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/9/bwv667.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/11/bwv0202.txt ; 11 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/7/bwv668.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/7/bwv1041b.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/7/bwv988.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/BOURREE.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0243.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/GIGUE.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/GAVOTTE.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/rejouiss.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bjsbmm23.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bjsbmm20.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bjsbmm11.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0205.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/OVERTURE.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248d.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248e.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248f.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248b.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248c.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/16/bwv0248a.txt ; 16 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/bwv655.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/bwv657.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/jsb212sf.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/bwv1076.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/bjsbmm10.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/6/bwv1087.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv872.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv866.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv858.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv870.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv864.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv848.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv860.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv874.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/Wtcii13b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/Wtcii09b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv862.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv876.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv806.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv817.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv816.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/Wtcii15b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv815.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/1/vs3-4alg.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/1/cs6-3cou.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv809.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv846.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv852.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv850.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv878.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/1/vp3-6gig.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/Wtcii11b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv854.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv868.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv856.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/1/bwv880.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/10/bwv651.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/10/bwv0212.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/8/bwv656.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/8/bwv1072.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/8/AIR.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/8/bjsbmm05.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs6-5gav.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs6-6gig.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs6-4sar.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs4-5bou.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/4/bwv1028.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/4/bwv971a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv653.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs3-1pre.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/Wtcii05b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/pfa-2fug.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vp3-1pre.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv652.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs4-1pre.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv1074.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/988-v23.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/988-v30.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs1-4sar.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vp3-3gav.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/988-v16.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vs3-2fug.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/988-v10.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/988-v04.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/Wtcii17b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/Wtcii01b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vs2-3and.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs3-4sar.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv663.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs3-6gig.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv653-2.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv552p.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vs1-3sic.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/bwv662.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/cs3-2all.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/Wtcii07b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vp3-4min.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vs3-3lar.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/4/vs3-1ada.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/4/bwv664.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/15/bjsbmm18.txt ; 15 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/15/bjsbmm19.txt ; 15 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/15/bjsbmm04.txt ; 15 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/15/bjsbmm06.txt ; 15 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/15/bjsbmm16.txt ; 15 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bjs1031c.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_3b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_2d.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_3a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bjs1031a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_2a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_3d.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/3/sonat_2b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs1-2all.txt ; 3 voices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v09.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/pfa-1pre.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/vp3-2lou.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs1-6gig.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v24.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v19.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_4c.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bwv525-3.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs3-5bou.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/3/Wtcii03b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bwv525-1.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_6e.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v03.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v02.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs1-1pre.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs4-4sar.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/Wtcii21b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bwv0541p.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bwv0541f.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/988-v13.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_6a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs1-5men.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/sonat_1c.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/pfa-3alg.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/cs6-1pre.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/3/bwv1027.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv806j.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv809d.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv1075.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv804.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/988-v20.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/988-v08.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv809g.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv806h.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv803.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv808g.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv809c.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv809b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/988-v27.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv809a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/vp3-5bou.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv811g.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv807g.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv806c.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv806b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv806a.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv806e.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv806d.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/Wtcii01a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv810f.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv772.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv806f.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv1086.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/988-v07.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/2/bwv806g.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/cs1-3cou.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/major/2/bwv772a.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/13/Bburg1_3.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/13/bburg1_1.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/13/bburg14a.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/13/bjsbmm13.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/13/bburg14h.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/5/bwv654.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/5/bwv1077.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/5/bjsbmm17.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/5/bjsbmm02.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/5/bwv552f.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/14/MENUET.txt ; 14 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/14/COURANTE.txt ; 14 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/14/FORLANE.txt ; 14 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/major/14/PASSPIED.txt ; 14 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/9/bwv1062.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/9/SARABAND.txt ; 9 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/11/Bburg1_2.txt ; 11 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/bwv1041a.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/1080-c12.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/1079-02.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/bwv1041c.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/bjsbmm09.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/7/bwv1052a.txt ; 7 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/bwv805.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/bwv1041a.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/1079-03.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/bwv1041c.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/bwv661.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/6/bwv665.txt ; 6 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii08b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv867.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv873.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv865.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv871.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv859.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii12b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/vp1-4cod.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv875.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv861.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv849.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv877.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv863.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv903.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv811.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/vp1-6sad.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv971.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii14b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv810.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv812.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/fp-4bou.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/fp-2cou.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv813.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv807.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv814.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii16b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/vp1-2ald.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/cs5-4sar.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/cs5-5gav.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/cs5-6gig.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/cs4-6gig.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/fp-3sar.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv808.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv853.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv847.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii10b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv879.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv851.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv869.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv855.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/vp2-4gig.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv881.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/1/fp-1all.txt ; 1 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/Wtcii11a.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/1/bwv857.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/10/RONDEAU.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/10/bwv1063.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/10/bjsbmm08.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/10/POLONAIS.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/10/BADINERI.txt ; 10 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/8/bwv1078.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/1080c02b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vp2-3sar.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/4/bwv982js.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vs2-1gra.txt ; 4 voices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs2-1pre.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs6-2all.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vp1-7tb.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs5-1pre.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vp2-1all.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv971.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/988-v22.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv582.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vs1-1ada.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/Wtcii02b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/Wtcii22b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vp1-5sa.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bjsbmm22.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv1074i.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv1073.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/1080-c02.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/Wtcii23b.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bjsbmm21.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/1079-04.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/1080-c01.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs3-3cou.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs4-2all.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vs2-2fug.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/988-v28.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv588.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs2-4sar.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/997-2fug.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/cs2-2all.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/bwv539.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/988-aria.txt ; 4 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/4/vp1-1al.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/15/bjsbmm03.txt ; 15 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii24b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv537.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_3c.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bjs1031b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv1029.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii04b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/cs4-3cou.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv733.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/vp1-3co.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_2c.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/3/bwv686.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_4d.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_5a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv630sc.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/1079-02.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_5c.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v21.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_5b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii18b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv525-2.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii19b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_4b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v18.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v25.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_4a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/cs2-5men.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v26.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_5d.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/1079-05.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/cs2-3cou.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv801.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_6d.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v15.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii20b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/vs1-4prs.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/3/sonat_6b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v12.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/988-v06.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/bwv660.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_1b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/cs2-6gig.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/Wtcii06b.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_1a.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/sonat_1d.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/3/bwv1030.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/vp2-2cou.txt ; 3 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/3/vp1-8tbd.txt ; 3 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/12/bjsbmm15.txt ; 12 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv906a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/Wtcii24a.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv906b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/cs5-3cou.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv805.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv809e.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv808a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv582.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv630sc.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/1080-c12.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv806i.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv808b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv809f.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv808c.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/1079-03.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv811h.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv807h.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv808f.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/997-4gig.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv802.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/1079-04.txt ; 1 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv784.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv808d.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv808e.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv810c.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/cs5-2all.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv808h.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v17.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv807f.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv811f.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv810b.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv811d.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv807d.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v14.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v01.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v29.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv807e.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv810a.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv811e.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv811a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv810e.txt ; 2 voices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: ../../ml/music_rnn/txt/minor/2/vs2-4alg.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/sonat_6c.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv807a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v05.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/988-v11.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/997-1pre.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv810d.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv811b.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv805th.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv807b.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv807c.txt ; 2 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv811c.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv810g.txt ; 8 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/Wtcii10a.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/997-5dou.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/2/bwv666.txt ; 8 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/2/bwv658.txt ; 2 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/13/bwv_1060.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/13/bjsbmm01.txt ; 13 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/5/vs1-2fug.txt ; 5 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/5/bwv1063.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/5/vp2-5cha.txt ; 5 voices\n",
      "Loading file: ../../ml/music_rnn/txt/minor/5/bwv1062.txt ; 4 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/5/997-3sar.txt ; 5 voices\n",
      "Skipping file: ../../ml/music_rnn/txt/minor/5/bwv659.txt ; 5 voices\n"
     ]
    }
   ],
   "source": [
    "PATH = \"../../ml/music_rnn/txt\"\n",
    "\n",
    "data, codetext, valitext, opusranges, count, dictionary, reverse_dictionary = read_data_files(PATH, validation=True)\n",
    "\n",
    "#data, count, dictionary, reverse_dictionary = build_dataset(dypthongs)\n",
    "#print('Most common words (+UNK)', count[:5])\n",
    "#print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: '    ',\n",
       " 2: 'X   ',\n",
       " 3: ' X  ',\n",
       " 4: '  X ',\n",
       " 5: '   X',\n",
       " 6: 'Q   ',\n",
       " 7: ' Q  ',\n",
       " 8: '  Q ',\n",
       " 9: '   Q',\n",
       " 10: 'S   ',\n",
       " 11: ' S  ',\n",
       " 12: '  S ',\n",
       " 13: '   S',\n",
       " 14: ']   ',\n",
       " 15: ' ]  ',\n",
       " 16: '  ] ',\n",
       " 17: '   ]',\n",
       " 18: 'L   ',\n",
       " 19: ' L  ',\n",
       " 20: '  L ',\n",
       " 21: '   L',\n",
       " 22: 'V   ',\n",
       " 23: ' V  ',\n",
       " 24: '  V ',\n",
       " 25: '   V',\n",
       " 26: '_   ',\n",
       " 27: ' _  ',\n",
       " 28: '  _ ',\n",
       " 29: '   _',\n",
       " 30: 'd   ',\n",
       " 31: ' d  ',\n",
       " 32: '  d ',\n",
       " 33: '   d',\n",
       " 34: 'T   ',\n",
       " 35: ' T  ',\n",
       " 36: '  T ',\n",
       " 37: '   T',\n",
       " 38: 'b   ',\n",
       " 39: ' b  ',\n",
       " 40: '  b ',\n",
       " 41: '   b',\n",
       " 42: 'U   ',\n",
       " 43: ' U  ',\n",
       " 44: '  U ',\n",
       " 45: '   U',\n",
       " 46: 'i   ',\n",
       " 47: ' i  ',\n",
       " 48: '  i ',\n",
       " 49: '   i',\n",
       " 50: 'P   ',\n",
       " 51: ' P  ',\n",
       " 52: '  P ',\n",
       " 53: '   P',\n",
       " 54: '`   ',\n",
       " 55: ' `  ',\n",
       " 56: '  ` ',\n",
       " 57: '   `',\n",
       " 58: 'N   ',\n",
       " 59: ' N  ',\n",
       " 60: '  N ',\n",
       " 61: '   N',\n",
       " 62: 'Z   ',\n",
       " 63: ' Z  ',\n",
       " 64: '  Z ',\n",
       " 65: '   Z',\n",
       " 66: '[   ',\n",
       " 67: ' [  ',\n",
       " 68: '  [ ',\n",
       " 69: '   [',\n",
       " 70: 'J   ',\n",
       " 71: ' J  ',\n",
       " 72: '  J ',\n",
       " 73: '   J',\n",
       " 74: 'Y   ',\n",
       " 75: ' Y  ',\n",
       " 76: '  Y ',\n",
       " 77: '   Y',\n",
       " 78: '\\\\   ',\n",
       " 79: ' \\\\  ',\n",
       " 80: '  \\\\ ',\n",
       " 81: '   \\\\',\n",
       " 82: 'k   ',\n",
       " 83: ' k  ',\n",
       " 84: '  k ',\n",
       " 85: '   k',\n",
       " 86: 'E   ',\n",
       " 87: ' E  ',\n",
       " 88: '  E ',\n",
       " 89: '   E',\n",
       " 90: 'f   ',\n",
       " 91: ' f  ',\n",
       " 92: '  f ',\n",
       " 93: '   f',\n",
       " 94: 'a   ',\n",
       " 95: ' a  ',\n",
       " 96: '  a ',\n",
       " 97: '   a',\n",
       " 98: 'O   ',\n",
       " 99: ' O  ',\n",
       " 100: '  O ',\n",
       " 101: '   O',\n",
       " 102: 'G   ',\n",
       " 103: ' G  ',\n",
       " 104: '  G ',\n",
       " 105: '   G',\n",
       " 106: 'g   ',\n",
       " 107: ' g  ',\n",
       " 108: '  g ',\n",
       " 109: '   g',\n",
       " 110: 'e   ',\n",
       " 111: ' e  ',\n",
       " 112: '  e ',\n",
       " 113: '   e',\n",
       " 114: 'h   ',\n",
       " 115: ' h  ',\n",
       " 116: '  h ',\n",
       " 117: '   h',\n",
       " 118: 'M   ',\n",
       " 119: ' M  ',\n",
       " 120: '  M ',\n",
       " 121: '   M',\n",
       " 122: 'I   ',\n",
       " 123: ' I  ',\n",
       " 124: '  I ',\n",
       " 125: '   I',\n",
       " 126: 'm   ',\n",
       " 127: ' m  ',\n",
       " 128: '  m ',\n",
       " 129: '   m',\n",
       " 130: 'H   ',\n",
       " 131: ' H  ',\n",
       " 132: '  H ',\n",
       " 133: '   H',\n",
       " 134: 'n   ',\n",
       " 135: ' n  ',\n",
       " 136: '  n ',\n",
       " 137: '   n',\n",
       " 138: 'p   ',\n",
       " 139: ' p  ',\n",
       " 140: '  p ',\n",
       " 141: '   p',\n",
       " 142: 'W   ',\n",
       " 143: ' W  ',\n",
       " 144: '  W ',\n",
       " 145: '   W',\n",
       " 146: '********',\n",
       " 147: 'l   ',\n",
       " 148: ' l  ',\n",
       " 149: '  l ',\n",
       " 150: '   l',\n",
       " 151: 'c   ',\n",
       " 152: ' c  ',\n",
       " 153: '  c ',\n",
       " 154: '   c',\n",
       " 155: '  @ ',\n",
       " 156: '   @',\n",
       " 157: ' @  ',\n",
       " 158: '@   ',\n",
       " 159: '^   ',\n",
       " 160: ' ^  ',\n",
       " 161: '  ^ ',\n",
       " 162: '   ^',\n",
       " 163: 'K   ',\n",
       " 164: ' K  ',\n",
       " 165: '  K ',\n",
       " 166: '   K',\n",
       " 167: 'R   ',\n",
       " 168: ' R  ',\n",
       " 169: '  R ',\n",
       " 170: '   R',\n",
       " 171: 'B   ',\n",
       " 172: ' B  ',\n",
       " 173: '  B ',\n",
       " 174: '   B',\n",
       " 175: 'C   ',\n",
       " 176: ' C  ',\n",
       " 177: '  C ',\n",
       " 178: '   C',\n",
       " 179: 'D   ',\n",
       " 180: ' D  ',\n",
       " 181: '  D ',\n",
       " 182: '   D',\n",
       " 183: 'j   ',\n",
       " 184: ' j  ',\n",
       " 185: '  j ',\n",
       " 186: '   j',\n",
       " 187: 'r   ',\n",
       " 188: ' r  ',\n",
       " 189: '  r ',\n",
       " 190: '   r',\n",
       " 191: 'o   ',\n",
       " 192: ' o  ',\n",
       " 193: '  o ',\n",
       " 194: '   o',\n",
       " 195: ' >  ',\n",
       " 196: '  > ',\n",
       " 197: '   >',\n",
       " 198: '>   ',\n",
       " 199: 'u   ',\n",
       " 200: ' u  ',\n",
       " 201: '  u ',\n",
       " 202: '   u',\n",
       " 203: 'q   ',\n",
       " 204: ' q  ',\n",
       " 205: '  q ',\n",
       " 206: '   q',\n",
       " 207: 'X Q ',\n",
       " 208: ' QX ',\n",
       " 209: 'XQ  ',\n",
       " 210: 'X  Q',\n",
       " 211: ' Q X',\n",
       " 212: ' XQ ',\n",
       " 213: ' X Q',\n",
       " 214: '  QX',\n",
       " 215: '  XQ',\n",
       " 216: 'Q X ',\n",
       " 217: 'QX  ',\n",
       " 218: 'Q  X',\n",
       " 219: ' QE ',\n",
       " 220: ' Q E',\n",
       " 221: ' EQ ',\n",
       " 222: ' E Q',\n",
       " 223: '  QE',\n",
       " 224: '  EQ',\n",
       " 225: 'Q E ',\n",
       " 226: 'Q  E',\n",
       " 227: 'QE  ',\n",
       " 228: 'E Q ',\n",
       " 229: 'E  Q',\n",
       " 230: 'EQ  ',\n",
       " 231: ']Q  ',\n",
       " 232: '] Q ',\n",
       " 233: ']  Q',\n",
       " 234: 'Q]  ',\n",
       " 235: ' ]Q ',\n",
       " 236: ' ] Q',\n",
       " 237: 'Q ] ',\n",
       " 238: ' Q] ',\n",
       " 239: 'Q  ]',\n",
       " 240: ' Q ]',\n",
       " 241: '  ]Q',\n",
       " 242: '  Q]',\n",
       " 243: 'aQ  ',\n",
       " 244: 'a Q ',\n",
       " 245: 'a  Q',\n",
       " 246: 'Qa  ',\n",
       " 247: 'Q a ',\n",
       " 248: 'Q  a',\n",
       " 249: ' aQ ',\n",
       " 250: ' a Q',\n",
       " 251: ' Qa ',\n",
       " 252: ' Q a',\n",
       " 253: '  aQ',\n",
       " 254: '  Qa',\n",
       " 255: ' E9 ',\n",
       " 256: ' E 9',\n",
       " 257: ' 9E ',\n",
       " 258: ' 9 E',\n",
       " 259: '  E9',\n",
       " 260: '  9E',\n",
       " 261: 'E 9 ',\n",
       " 262: 'E  9',\n",
       " 263: 'E9  ',\n",
       " 264: '9 E ',\n",
       " 265: '9  E',\n",
       " 266: '9E  ',\n",
       " 267: '\\\\ S ',\n",
       " 268: '\\\\S  ',\n",
       " 269: '\\\\  S',\n",
       " 270: ' \\\\S ',\n",
       " 271: 'S\\\\  ',\n",
       " 272: ' \\\\ S',\n",
       " 273: ' S\\\\ ',\n",
       " 274: 'S \\\\ ',\n",
       " 275: ' S \\\\',\n",
       " 276: 'S  \\\\',\n",
       " 277: '  \\\\S',\n",
       " 278: '  S\\\\',\n",
       " 279: ' L@ ',\n",
       " 280: ' L @',\n",
       " 281: ' @L ',\n",
       " 282: ' @ L',\n",
       " 283: '  L@',\n",
       " 284: '  @L',\n",
       " 285: 'L @ ',\n",
       " 286: 'L  @',\n",
       " 287: 'L@  ',\n",
       " 288: '@ L ',\n",
       " 289: '@  L',\n",
       " 290: '@L  ',\n",
       " 291: 't   ',\n",
       " 292: ' t  ',\n",
       " 293: '  t ',\n",
       " 294: '   t',\n",
       " 295: 'b S ',\n",
       " 296: 'b  S',\n",
       " 297: 'bS  ',\n",
       " 298: ' bS ',\n",
       " 299: ' b S',\n",
       " 300: ' Sb ',\n",
       " 301: ' S b',\n",
       " 302: '  bS',\n",
       " 303: '  Sb',\n",
       " 304: 'Sb  ',\n",
       " 305: 'S b ',\n",
       " 306: 'S  b',\n",
       " 307: 'T]  ',\n",
       " 308: 'T ] ',\n",
       " 309: 'T  ]',\n",
       " 310: ']T  ',\n",
       " 311: ' T] ',\n",
       " 312: ' T ]',\n",
       " 313: '] T ',\n",
       " 314: ' ]T ',\n",
       " 315: ']  T',\n",
       " 316: ' ] T',\n",
       " 317: '  T]',\n",
       " 318: '  ]T',\n",
       " 319: ' V_ ',\n",
       " 320: ' V _',\n",
       " 321: ' _V ',\n",
       " 322: ' _ V',\n",
       " 323: '  V_',\n",
       " 324: '  _V',\n",
       " 325: 'V _ ',\n",
       " 326: 'V  _',\n",
       " 327: 'V_  ',\n",
       " 328: '_ V ',\n",
       " 329: '_  V',\n",
       " 330: '_V  ',\n",
       " 331: 'SL  ',\n",
       " 332: 'LS  ',\n",
       " 333: 'S L ',\n",
       " 334: 'L S ',\n",
       " 335: 'S  L',\n",
       " 336: 'L  S',\n",
       " 337: ' SL ',\n",
       " 338: ' LS ',\n",
       " 339: ' S L',\n",
       " 340: ' L S',\n",
       " 341: '  SL',\n",
       " 342: '  LS',\n",
       " 343: '_X  ',\n",
       " 344: '_ X ',\n",
       " 345: '_  X',\n",
       " 346: 'X_  ',\n",
       " 347: 'X _ ',\n",
       " 348: 'X  _',\n",
       " 349: ' _X ',\n",
       " 350: ' _ X',\n",
       " 351: ' X_ ',\n",
       " 352: ' X _',\n",
       " 353: '  _X',\n",
       " 354: '  X_',\n",
       " 355: 'F   ',\n",
       " 356: ' F  ',\n",
       " 357: '  F ',\n",
       " 358: '   F',\n",
       " 359: 'X L ',\n",
       " 360: ' XL ',\n",
       " 361: 'X  L',\n",
       " 362: ' X L',\n",
       " 363: 'XL  ',\n",
       " 364: ' LX ',\n",
       " 365: ' L X',\n",
       " 366: '  XL',\n",
       " 367: '  LX',\n",
       " 368: 'LX  ',\n",
       " 369: 'L X ',\n",
       " 370: 'L  X',\n",
       " 371: 's   ',\n",
       " 372: ' s  ',\n",
       " 373: '  s ',\n",
       " 374: '   s',\n",
       " 375: '  A ',\n",
       " 376: '   A',\n",
       " 377: ' A  ',\n",
       " 378: 'A   ',\n",
       " 379: ']U  ',\n",
       " 380: '] U ',\n",
       " 381: ']  U',\n",
       " 382: 'U]  ',\n",
       " 383: ' ]U ',\n",
       " 384: ' ] U',\n",
       " 385: 'U ] ',\n",
       " 386: ' U] ',\n",
       " 387: 'U  ]',\n",
       " 388: ' U ]',\n",
       " 389: '  ]U',\n",
       " 390: '  U]',\n",
       " 391: ' I= ',\n",
       " 392: ' I =',\n",
       " 393: ' =I ',\n",
       " 394: ' = I',\n",
       " 395: '  I=',\n",
       " 396: '  =I',\n",
       " 397: 'I = ',\n",
       " 398: 'I  =',\n",
       " 399: 'I=  ',\n",
       " 400: '= I ',\n",
       " 401: '=  I',\n",
       " 402: '=I  ',\n",
       " 403: 'dQ  ',\n",
       " 404: 'd Q ',\n",
       " 405: 'd  Q',\n",
       " 406: 'Qd  ',\n",
       " 407: 'Q d ',\n",
       " 408: 'Q  d',\n",
       " 409: ' dQ ',\n",
       " 410: ' d Q',\n",
       " 411: ' Qd ',\n",
       " 412: ' Q d',\n",
       " 413: '  dQ',\n",
       " 414: '  Qd',\n",
       " 415: '`Q  ',\n",
       " 416: '` Q ',\n",
       " 417: '`  Q',\n",
       " 418: 'Q`  ',\n",
       " 419: 'Q ` ',\n",
       " 420: 'Q  `',\n",
       " 421: ' `Q ',\n",
       " 422: ' ` Q',\n",
       " 423: ' Q` ',\n",
       " 424: ' Q `',\n",
       " 425: '  `Q',\n",
       " 426: '  Q`',\n",
       " 427: 'd]  ',\n",
       " 428: 'd ] ',\n",
       " 429: 'd  ]',\n",
       " 430: ']d  ',\n",
       " 431: '] d ',\n",
       " 432: ']  d',\n",
       " 433: ' d] ',\n",
       " 434: ' d ]',\n",
       " 435: ' ]d ',\n",
       " 436: ' ] d',\n",
       " 437: '  d]',\n",
       " 438: '  ]d',\n",
       " 439: ' G; ',\n",
       " 440: ' G ;',\n",
       " 441: ' ;G ',\n",
       " 442: ' ; G',\n",
       " 443: '  G;',\n",
       " 444: '  ;G',\n",
       " 445: 'G ; ',\n",
       " 446: 'G  ;',\n",
       " 447: 'G;  ',\n",
       " 448: '; G ',\n",
       " 449: ';  G',\n",
       " 450: ';G  ',\n",
       " 451: ' dX ',\n",
       " 452: ' d X',\n",
       " 453: ' Xd ',\n",
       " 454: ' X d',\n",
       " 455: '  dX',\n",
       " 456: '  Xd',\n",
       " 457: 'd X ',\n",
       " 458: 'd  X',\n",
       " 459: 'dX  ',\n",
       " 460: 'X d ',\n",
       " 461: 'X  d',\n",
       " 462: 'Xd  ',\n",
       " 463: 'd\\\\  ',\n",
       " 464: 'd \\\\ ',\n",
       " 465: 'd  \\\\',\n",
       " 466: '\\\\d  ',\n",
       " 467: '\\\\ d ',\n",
       " 468: '\\\\  d',\n",
       " 469: ' d\\\\ ',\n",
       " 470: ' d \\\\',\n",
       " 471: ' \\\\d ',\n",
       " 472: ' \\\\ d',\n",
       " 473: '  d\\\\',\n",
       " 474: '  \\\\d',\n",
       " 475: 'X ` ',\n",
       " 476: 'X  `',\n",
       " 477: 'X`  ',\n",
       " 478: ' X` ',\n",
       " 479: ' X `',\n",
       " 480: ' `X ',\n",
       " 481: ' ` X',\n",
       " 482: '  X`',\n",
       " 483: '  `X',\n",
       " 484: '`X  ',\n",
       " 485: '` X ',\n",
       " 486: '`  X',\n",
       " 487: 'w   ',\n",
       " 488: ' w  ',\n",
       " 489: '  w ',\n",
       " 490: '   w',\n",
       " 491: 'a X ',\n",
       " 492: 'aX  ',\n",
       " 493: 'a  X',\n",
       " 494: ' aX ',\n",
       " 495: 'Xa  ',\n",
       " 496: ' a X',\n",
       " 497: ' Xa ',\n",
       " 498: 'X a ',\n",
       " 499: ' X a',\n",
       " 500: 'X  a',\n",
       " 501: '  aX',\n",
       " 502: '  Xa',\n",
       " 503: ' _L ',\n",
       " 504: '_ L ',\n",
       " 505: ' _ L',\n",
       " 506: '_  L',\n",
       " 507: ' L_ ',\n",
       " 508: '_L  ',\n",
       " 509: ' L _',\n",
       " 510: '  _L',\n",
       " 511: '  L_',\n",
       " 512: 'L _ ',\n",
       " 513: 'L_  ',\n",
       " 514: 'L  _',\n",
       " 515: '`]  ',\n",
       " 516: '` ] ',\n",
       " 517: '`  ]',\n",
       " 518: ']`  ',\n",
       " 519: '] ` ',\n",
       " 520: ']  `',\n",
       " 521: ' `] ',\n",
       " 522: ' ` ]',\n",
       " 523: ' ]` ',\n",
       " 524: ' ] `',\n",
       " 525: '  `]',\n",
       " 526: '  ]`',\n",
       " 527: 'ia  ',\n",
       " 528: 'i a ',\n",
       " 529: 'i  a',\n",
       " 530: 'ai  ',\n",
       " 531: 'a i ',\n",
       " 532: 'a  i',\n",
       " 533: ' ia ',\n",
       " 534: ' i a',\n",
       " 535: ' ai ',\n",
       " 536: ' a i',\n",
       " 537: '  ia',\n",
       " 538: '  ai',\n",
       " 539: '] S ',\n",
       " 540: ']S  ',\n",
       " 541: ']  S',\n",
       " 542: ' ]S ',\n",
       " 543: 'S]  ',\n",
       " 544: ' ] S',\n",
       " 545: ' S] ',\n",
       " 546: 'S ] ',\n",
       " 547: ' S ]',\n",
       " 548: 'S  ]',\n",
       " 549: '  ]S',\n",
       " 550: '  S]',\n",
       " 551: 'dT  ',\n",
       " 552: 'd T ',\n",
       " 553: 'd  T',\n",
       " 554: 'Td  ',\n",
       " 555: 'T d ',\n",
       " 556: 'T  d',\n",
       " 557: ' dT ',\n",
       " 558: ' d T',\n",
       " 559: ' Td ',\n",
       " 560: ' T d',\n",
       " 561: '  dT',\n",
       " 562: '  Td',\n",
       " 563: 'da  ',\n",
       " 564: 'd a ',\n",
       " 565: 'd  a',\n",
       " 566: 'ad  ',\n",
       " 567: 'a d ',\n",
       " 568: 'a  d',\n",
       " 569: ' da ',\n",
       " 570: ' d a',\n",
       " 571: ' ad ',\n",
       " 572: ' a d',\n",
       " 573: '  da',\n",
       " 574: '  ad',\n",
       " 575: ' dU ',\n",
       " 576: 'd U ',\n",
       " 577: ' d U',\n",
       " 578: 'd  U',\n",
       " 579: ' Ud ',\n",
       " 580: 'dU  ',\n",
       " 581: ' U d',\n",
       " 582: '  dU',\n",
       " 583: '  Ud',\n",
       " 584: 'U d ',\n",
       " 585: 'Ud  ',\n",
       " 586: 'U  d',\n",
       " 587: 'h X ',\n",
       " 588: 'h  X',\n",
       " 589: 'hX  ',\n",
       " 590: ' hX ',\n",
       " 591: ' h X',\n",
       " 592: ' Xh ',\n",
       " 593: ' X h',\n",
       " 594: '  hX',\n",
       " 595: '  Xh',\n",
       " 596: 'Xh  ',\n",
       " 597: 'X h ',\n",
       " 598: 'X  h',\n",
       " 599: 'kb  ',\n",
       " 600: 'k b ',\n",
       " 601: 'k  b',\n",
       " 602: 'bk  ',\n",
       " 603: 'b k ',\n",
       " 604: 'b  k',\n",
       " 605: ' kb ',\n",
       " 606: ' k b',\n",
       " 607: ' bk ',\n",
       " 608: ' b k',\n",
       " 609: '  kb',\n",
       " 610: '  bk',\n",
       " 611: '\\\\L  ',\n",
       " 612: '\\\\ L ',\n",
       " 613: ' \\\\L ',\n",
       " 614: '\\\\  L',\n",
       " 615: ' \\\\ L',\n",
       " 616: ' L\\\\ ',\n",
       " 617: ' L \\\\',\n",
       " 618: '  \\\\L',\n",
       " 619: '  L\\\\',\n",
       " 620: 'L\\\\  ',\n",
       " 621: 'L \\\\ ',\n",
       " 622: 'L  \\\\',\n",
       " 623: 'XU  ',\n",
       " 624: 'X U ',\n",
       " 625: 'X  U',\n",
       " 626: 'UX  ',\n",
       " 627: ' XU ',\n",
       " 628: ' X U',\n",
       " 629: 'U X ',\n",
       " 630: ' UX ',\n",
       " 631: 'U  X',\n",
       " 632: ' U X',\n",
       " 633: '  XU',\n",
       " 634: '  UX',\n",
       " 635: 'i]  ',\n",
       " 636: 'i ] ',\n",
       " 637: 'i  ]',\n",
       " 638: ']i  ',\n",
       " 639: '] i ',\n",
       " 640: ']  i',\n",
       " 641: ' i] ',\n",
       " 642: ' i ]',\n",
       " 643: ' ]i ',\n",
       " 644: ' ] i',\n",
       " 645: '  i]',\n",
       " 646: '  ]i',\n",
       " 647: 'Z Q ',\n",
       " 648: 'Z  Q',\n",
       " 649: 'ZQ  ',\n",
       " 650: ' ZQ ',\n",
       " 651: ' Z Q',\n",
       " 652: ' QZ ',\n",
       " 653: ' Q Z',\n",
       " 654: '  ZQ',\n",
       " 655: '  QZ',\n",
       " 656: 'QZ  ',\n",
       " 657: 'Q Z ',\n",
       " 658: 'Q  Z',\n",
       " 659: 'b_  ',\n",
       " 660: 'b _ ',\n",
       " 661: 'b  _',\n",
       " 662: '_b  ',\n",
       " 663: '_ b ',\n",
       " 664: '_  b',\n",
       " 665: ' b_ ',\n",
       " 666: ' b _',\n",
       " 667: ' _b ',\n",
       " 668: ' _ b',\n",
       " 669: '  b_',\n",
       " 670: '  _b',\n",
       " 671: ' a] ',\n",
       " 672: ' a ]',\n",
       " 673: ' ]a ',\n",
       " 674: ' ] a',\n",
       " 675: '  a]',\n",
       " 676: '  ]a',\n",
       " 677: 'a ] ',\n",
       " 678: 'a  ]',\n",
       " 679: 'a]  ',\n",
       " 680: '] a ',\n",
       " 681: ']  a',\n",
       " 682: ']a  ',\n",
       " 683: ' J> ',\n",
       " 684: ' J >',\n",
       " 685: ' >J ',\n",
       " 686: ' > J',\n",
       " 687: '  J>',\n",
       " 688: '  >J',\n",
       " 689: 'J > ',\n",
       " 690: 'J  >',\n",
       " 691: 'J>  ',\n",
       " 692: '> J ',\n",
       " 693: '>  J',\n",
       " 694: '>J  ',\n",
       " 695: '  < ',\n",
       " 696: '   <',\n",
       " 697: ' <  ',\n",
       " 698: '<   ',\n",
       " 699: 'bZ  ',\n",
       " 700: 'b Z ',\n",
       " 701: 'b  Z',\n",
       " 702: 'Zb  ',\n",
       " 703: 'Z b ',\n",
       " 704: 'Z  b',\n",
       " 705: ' bZ ',\n",
       " 706: ' b Z',\n",
       " 707: ' Zb ',\n",
       " 708: ' Z b',\n",
       " 709: '  bZ',\n",
       " 710: '  Zb',\n",
       " 711: 'd L ',\n",
       " 712: 'd  L',\n",
       " 713: 'dL  ',\n",
       " 714: ' dL ',\n",
       " 715: ' d L',\n",
       " 716: ' Ld ',\n",
       " 717: ' L d',\n",
       " 718: '  dL',\n",
       " 719: '  Ld',\n",
       " 720: 'Ld  ',\n",
       " 721: 'L d ',\n",
       " 722: 'L  d',\n",
       " 723: ' XT ',\n",
       " 724: 'TX  ',\n",
       " 725: ' TQ ',\n",
       " 726: ' X T',\n",
       " 727: ' T Q',\n",
       " 728: ' TX ',\n",
       " 729: 'T X ',\n",
       " 730: ' QT ',\n",
       " 731: ' T X',\n",
       " 732: 'T  X',\n",
       " 733: ' Q T',\n",
       " 734: '  XT',\n",
       " 735: '  TQ',\n",
       " 736: '  TX',\n",
       " 737: '  QT',\n",
       " 738: 'X T ',\n",
       " 739: 'XT  ',\n",
       " 740: 'T Q ',\n",
       " 741: 'X  T',\n",
       " 742: 'T  Q',\n",
       " 743: 'TQ  ',\n",
       " 744: 'Q T ',\n",
       " 745: 'Q  T',\n",
       " 746: 'QT  ',\n",
       " 747: '] N ',\n",
       " 748: ']  N',\n",
       " 749: ']N  ',\n",
       " 750: ' ]N ',\n",
       " 751: ' ] N',\n",
       " 752: ' N] ',\n",
       " 753: ' N ]',\n",
       " 754: '  ]N',\n",
       " 755: '  N]',\n",
       " 756: 'N]  ',\n",
       " 757: 'N ] ',\n",
       " 758: 'N  ]',\n",
       " 759: 'd P ',\n",
       " 760: 'dP  ',\n",
       " 761: '_ P ',\n",
       " 762: 'd  P',\n",
       " 763: '_  P',\n",
       " 764: '_P  ',\n",
       " 765: ' dP ',\n",
       " 766: 'Pd  ',\n",
       " 767: ' _P ',\n",
       " 768: ' d P',\n",
       " 769: ' _ P',\n",
       " 770: ' Pd ',\n",
       " 771: 'P d ',\n",
       " 772: ' P_ ',\n",
       " 773: ' P d',\n",
       " 774: 'P  d',\n",
       " 775: ' P _',\n",
       " 776: '  dP',\n",
       " 777: '  _P',\n",
       " 778: '  Pd',\n",
       " 779: '  P_',\n",
       " 780: 'P_  ',\n",
       " 781: 'P _ ',\n",
       " 782: 'P  _',\n",
       " 783: 'XP  ',\n",
       " 784: 'X P ',\n",
       " 785: 'X  P',\n",
       " 786: 'PX  ',\n",
       " 787: 'P X ',\n",
       " 788: 'P  X',\n",
       " 789: ' XP ',\n",
       " 790: ' X P',\n",
       " 791: ' PX ',\n",
       " 792: ' P X',\n",
       " 793: '  XP',\n",
       " 794: '  PX',\n",
       " 795: ' \\\\X ',\n",
       " 796: ' \\\\ X',\n",
       " 797: ' X\\\\ ',\n",
       " 798: ' X \\\\',\n",
       " 799: '  \\\\X',\n",
       " 800: '  X\\\\',\n",
       " 801: '\\\\ X ',\n",
       " 802: '\\\\  X',\n",
       " 803: '\\\\X  ',\n",
       " 804: 'X \\\\ ',\n",
       " 805: 'X  \\\\',\n",
       " 806: 'X\\\\  ',\n",
       " 807: 'ZV  ',\n",
       " 808: 'Z V ',\n",
       " 809: 'Z  V',\n",
       " 810: 'VZ  ',\n",
       " 811: ' ZV ',\n",
       " 812: ' Z V',\n",
       " 813: 'V Z ',\n",
       " 814: ' VZ ',\n",
       " 815: 'V  Z',\n",
       " 816: ' V Z',\n",
       " 817: '  ZV',\n",
       " 818: '  VZ',\n",
       " 819: ' NB ',\n",
       " 820: ' N B',\n",
       " 821: ' BN ',\n",
       " 822: ' B N',\n",
       " 823: '  NB',\n",
       " 824: '  BN',\n",
       " 825: 'N B ',\n",
       " 826: 'N  B',\n",
       " 827: 'NB  ',\n",
       " 828: 'B N ',\n",
       " 829: 'B  N',\n",
       " 830: 'BN  ',\n",
       " 831: ' mi ',\n",
       " 832: ' m i',\n",
       " 833: ' im ',\n",
       " 834: ' i m',\n",
       " 835: '  mi',\n",
       " 836: '  im',\n",
       " 837: 'm i ',\n",
       " 838: 'm  i',\n",
       " 839: 'mi  ',\n",
       " 840: 'i m ',\n",
       " 841: 'i  m',\n",
       " 842: 'im  ',\n",
       " 843: 'g X ',\n",
       " 844: 'g  X',\n",
       " 845: 'gX  ',\n",
       " 846: ' gX ',\n",
       " 847: ' g X',\n",
       " 848: ' Xg ',\n",
       " 849: ' X g',\n",
       " 850: '  gX',\n",
       " 851: '  Xg',\n",
       " 852: 'Xg  ',\n",
       " 853: 'X g ',\n",
       " 854: 'X  g',\n",
       " 855: 'md  ',\n",
       " 856: 'm d ',\n",
       " 857: 'm  d',\n",
       " 858: 'dm  ',\n",
       " 859: 'd m ',\n",
       " 860: 'd  m',\n",
       " 861: ' md ',\n",
       " 862: ' m d',\n",
       " 863: ' dm ',\n",
       " 864: ' d m',\n",
       " 865: '  md',\n",
       " 866: '  dm',\n",
       " 867: 'fb  ',\n",
       " 868: 'f b ',\n",
       " 869: 'f  b',\n",
       " 870: 'bf  ',\n",
       " 871: 'b f ',\n",
       " 872: 'b  f',\n",
       " 873: ' fb ',\n",
       " 874: ' f b',\n",
       " 875: ' bf ',\n",
       " 876: ' b f',\n",
       " 877: '  fb',\n",
       " 878: '  bf',\n",
       " 879: 'd`  ',\n",
       " 880: 'd ` ',\n",
       " 881: 'd  `',\n",
       " 882: '`d  ',\n",
       " 883: '` d ',\n",
       " 884: '`  d',\n",
       " 885: ' d` ',\n",
       " 886: ' d `',\n",
       " 887: ' `d ',\n",
       " 888: ' ` d',\n",
       " 889: '  d`',\n",
       " 890: '  `d',\n",
       " 891: 'd[  ',\n",
       " 892: 'd [ ',\n",
       " 893: 'd  [',\n",
       " 894: '[d  ',\n",
       " 895: '[ d ',\n",
       " 896: '[  d',\n",
       " 897: ' d[ ',\n",
       " 898: ' d [',\n",
       " 899: ' [d ',\n",
       " 900: ' [ d',\n",
       " 901: '  d[',\n",
       " 902: '  [d',\n",
       " 903: 'f]  ',\n",
       " 904: 'f ] ',\n",
       " 905: 'f  ]',\n",
       " 906: ']f  ',\n",
       " 907: '] f ',\n",
       " 908: ']  f',\n",
       " 909: ' f] ',\n",
       " 910: ' f ]',\n",
       " 911: ' ]f ',\n",
       " 912: ' ] f',\n",
       " 913: '  f]',\n",
       " 914: '  ]f',\n",
       " 915: ' h_ ',\n",
       " 916: ' h _',\n",
       " 917: ' _h ',\n",
       " 918: ' _ h',\n",
       " 919: '  h_',\n",
       " 920: '  _h',\n",
       " 921: 'h _ ',\n",
       " 922: 'h  _',\n",
       " 923: 'h_  ',\n",
       " 924: '_ h ',\n",
       " 925: '_  h',\n",
       " 926: '_h  ',\n",
       " 927: ' if ',\n",
       " 928: ' hd ',\n",
       " 929: 'hd  ',\n",
       " 930: ' i f',\n",
       " 931: ' h d',\n",
       " 932: ' fi ',\n",
       " 933: ' dh ',\n",
       " 934: 'h d ',\n",
       " 935: ' f i',\n",
       " 936: ' d h',\n",
       " 937: 'h  d',\n",
       " 938: '  if',\n",
       " 939: '  hd',\n",
       " 940: '  fi',\n",
       " 941: '  dh',\n",
       " 942: 'i f ',\n",
       " 943: 'dh  ',\n",
       " 944: 'i  f',\n",
       " 945: 'if  ',\n",
       " 946: 'd h ',\n",
       " 947: 'd  h',\n",
       " 948: 'f i ',\n",
       " 949: 'f  i',\n",
       " 950: 'fi  ',\n",
       " 951: ']X  ',\n",
       " 952: '_Q  ',\n",
       " 953: '] X ',\n",
       " 954: '_ Q ',\n",
       " 955: ']  X',\n",
       " 956: '_  Q',\n",
       " 957: 'X]  ',\n",
       " 958: 'Q_  ',\n",
       " 959: 'X ] ',\n",
       " 960: 'Q _ ',\n",
       " 961: 'X  ]',\n",
       " 962: 'Q  _',\n",
       " 963: ' ]X ',\n",
       " 964: ' _Q ',\n",
       " 965: ' ] X',\n",
       " 966: ' _ Q',\n",
       " 967: ' X] ',\n",
       " 968: ' Q_ ',\n",
       " 969: ' X ]',\n",
       " 970: ' Q _',\n",
       " 971: '  ]X',\n",
       " 972: '  _Q',\n",
       " 973: '  X]',\n",
       " 974: '  Q_',\n",
       " 975: ']V  ',\n",
       " 976: '] V ',\n",
       " 977: ']  V',\n",
       " 978: 'V]  ',\n",
       " 979: 'V ] ',\n",
       " 980: 'V  ]',\n",
       " 981: ' ]V ',\n",
       " 982: ' ] V',\n",
       " 983: ' V] ',\n",
       " 984: ' V ]',\n",
       " 985: '  ]V',\n",
       " 986: '  V]',\n",
       " 987: '_ J ',\n",
       " 988: '_  J',\n",
       " 989: '_J  ',\n",
       " 990: ' _J ',\n",
       " 991: ' _ J',\n",
       " 992: ' J_ ',\n",
       " 993: ' J _',\n",
       " 994: '  _J',\n",
       " 995: '  J_',\n",
       " 996: 'J_  ',\n",
       " 997: 'J _ ',\n",
       " 998: 'J  _',\n",
       " 999: 'bQ  ',\n",
       " ...}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, None, 256)         271360    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, None, 8)           2056      \n",
      "=================================================================\n",
      "Total params: 1,324,040\n",
      "Trainable params: 1,324,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "5914089\n",
      "Traning epoch: 0 / batch:0, samples: 0\n",
      " Train Data Loss: 0.21594806015491486, Accuracy: 0.13671875\n",
      "92583\n",
      "32/32 [==============================] - 1s\n",
      " Validation Loss: 0.11447607725858688, Accuracy: 0.2138671875\n",
      "Traning epoch: 0 / batch:50, samples: 102400\n",
      " Train Data Loss: 0.07468955218791962, Accuracy: 0.44091796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.051486529409885406, Accuracy: 0.50537109375\n",
      "Traning epoch: 0 / batch:100, samples: 204800\n",
      " Train Data Loss: 0.06901997327804565, Accuracy: 0.552734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.046616777777671814, Accuracy: 0.54345703125\n",
      "Traning epoch: 0 / batch:150, samples: 307200\n",
      " Train Data Loss: 0.06657947599887848, Accuracy: 0.58349609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04574228823184967, Accuracy: 0.544921875\n",
      "Traning epoch: 0 / batch:200, samples: 409600\n",
      " Train Data Loss: 0.05620363727211952, Accuracy: 0.6015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.042542796581983566, Accuracy: 0.5517578125\n",
      "Traning epoch: 0 / batch:250, samples: 512000\n",
      " Train Data Loss: 0.057254113256931305, Accuracy: 0.5546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04271281510591507, Accuracy: 0.54345703125\n",
      "Traning epoch: 0 / batch:300, samples: 614400\n",
      " Train Data Loss: 0.04759412258863449, Accuracy: 0.71142578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04221571236848831, Accuracy: 0.54443359375\n",
      "Traning epoch: 0 / batch:350, samples: 716800\n",
      " Train Data Loss: 0.05033862218260765, Accuracy: 0.5751953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.043130844831466675, Accuracy: 0.52001953125\n",
      "Traning epoch: 0 / batch:400, samples: 819200\n",
      " Train Data Loss: 0.04707830399274826, Accuracy: 0.49072265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04273775592446327, Accuracy: 0.5244140625\n",
      "Traning epoch: 0 / batch:450, samples: 921600\n",
      " Train Data Loss: 0.04211180657148361, Accuracy: 0.6220703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04156572371721268, Accuracy: 0.5380859375\n",
      "Traning epoch: 0 / batch:500, samples: 1024000\n",
      " Train Data Loss: 0.03967957943677902, Accuracy: 0.548828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0434986874461174, Accuracy: 0.53466796875\n",
      "Traning epoch: 0 / batch:550, samples: 1126400\n",
      " Train Data Loss: 0.02742224931716919, Accuracy: 0.5947265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04239644855260849, Accuracy: 0.53955078125\n",
      "Traning epoch: 0 / batch:600, samples: 1228800\n",
      " Train Data Loss: 0.025221915915608406, Accuracy: 0.5458984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04244000464677811, Accuracy: 0.60693359375\n",
      "Traning epoch: 0 / batch:650, samples: 1331200\n",
      " Train Data Loss: 0.033004581928253174, Accuracy: 0.70068359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04223461076617241, Accuracy: 0.63720703125\n",
      "Traning epoch: 0 / batch:700, samples: 1433600\n",
      " Train Data Loss: 0.03046116605401039, Accuracy: 0.68408203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04262014478445053, Accuracy: 0.619140625\n",
      "Traning epoch: 0 / batch:750, samples: 1536000\n",
      " Train Data Loss: 0.03621671348810196, Accuracy: 0.6201171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.041528426110744476, Accuracy: 0.61328125\n",
      "Traning epoch: 0 / batch:800, samples: 1638400\n",
      " Train Data Loss: 0.030211035162210464, Accuracy: 0.6455078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04351484030485153, Accuracy: 0.64111328125\n",
      "Traning epoch: 0 / batch:850, samples: 1740800\n",
      " Train Data Loss: 0.02462131902575493, Accuracy: 0.65576171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04536488652229309, Accuracy: 0.62646484375\n",
      "Traning epoch: 0 / batch:900, samples: 1843200\n",
      " Train Data Loss: 0.03307139500975609, Accuracy: 0.66748046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04160240292549133, Accuracy: 0.646484375\n",
      "Traning epoch: 0 / batch:950, samples: 1945600\n",
      " Train Data Loss: 0.031454913318157196, Accuracy: 0.7001953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04267403110861778, Accuracy: 0.63134765625\n",
      "Traning epoch: 0 / batch:1000, samples: 2048000\n",
      " Train Data Loss: 0.03516440838575363, Accuracy: 0.720703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04203762859106064, Accuracy: 0.62744140625\n",
      "Traning epoch: 0 / batch:1050, samples: 2150400\n",
      " Train Data Loss: 0.038273513317108154, Accuracy: 0.76611328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04120858758687973, Accuracy: 0.65771484375\n",
      "Traning epoch: 0 / batch:1100, samples: 2252800\n",
      " Train Data Loss: 0.027861660346388817, Accuracy: 0.7353515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04151301831007004, Accuracy: 0.6435546875\n",
      "Traning epoch: 0 / batch:1150, samples: 2355200\n",
      " Train Data Loss: 0.024750689044594765, Accuracy: 0.720703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.04109922796487808, Accuracy: 0.580078125\n",
      "Traning epoch: 0 / batch:1200, samples: 2457600\n",
      " Train Data Loss: 0.02681320160627365, Accuracy: 0.556640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.03465709835290909, Accuracy: 0.60107421875\n",
      "Traning epoch: 0 / batch:1250, samples: 2560000\n",
      " Train Data Loss: 0.02839427813887596, Accuracy: 0.60205078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.030539706349372864, Accuracy: 0.66650390625\n",
      "Traning epoch: 0 / batch:1300, samples: 2662400\n",
      " Train Data Loss: 0.03210017830133438, Accuracy: 0.603515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02664537914097309, Accuracy: 0.59716796875\n",
      "Traning epoch: 0 / batch:1350, samples: 2764800\n",
      " Train Data Loss: 0.02345597930252552, Accuracy: 0.56640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02565012499690056, Accuracy: 0.6376953125\n",
      "Traning epoch: 0 / batch:1400, samples: 2867200\n",
      " Train Data Loss: 0.02741606906056404, Accuracy: 0.564453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.024202825501561165, Accuracy: 0.609375\n",
      "Traning epoch: 0 / batch:1450, samples: 2969600\n",
      " Train Data Loss: 0.026740916073322296, Accuracy: 0.650390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.024759631603956223, Accuracy: 0.626953125\n",
      "Traning epoch: 0 / batch:1500, samples: 3072000\n",
      " Train Data Loss: 0.021329689770936966, Accuracy: 0.67626953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.026626823469996452, Accuracy: 0.6396484375\n",
      "Traning epoch: 0 / batch:1550, samples: 3174400\n",
      " Train Data Loss: 0.022186478599905968, Accuracy: 0.7001953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.024895790964365005, Accuracy: 0.62548828125\n",
      "Traning epoch: 0 / batch:1600, samples: 3276800\n",
      " Train Data Loss: 0.022921904921531677, Accuracy: 0.6962890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02429087832570076, Accuracy: 0.6123046875\n",
      "Traning epoch: 0 / batch:1650, samples: 3379200\n",
      " Train Data Loss: 0.022248927503824234, Accuracy: 0.6318359375\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.025008469820022583, Accuracy: 0.6591796875\n",
      "Traning epoch: 0 / batch:1700, samples: 3481600\n",
      " Train Data Loss: 0.02235705778002739, Accuracy: 0.587890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.024439290165901184, Accuracy: 0.62353515625\n",
      "Traning epoch: 0 / batch:1750, samples: 3584000\n",
      " Train Data Loss: 0.026988986879587173, Accuracy: 0.76708984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02483709342777729, Accuracy: 0.6337890625\n",
      "Traning epoch: 0 / batch:1800, samples: 3686400\n",
      " Train Data Loss: 0.026462597772479057, Accuracy: 0.66259765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.023004677146673203, Accuracy: 0.6181640625\n",
      "Traning epoch: 0 / batch:1850, samples: 3788800\n",
      " Train Data Loss: 0.031363703310489655, Accuracy: 0.57177734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.023420272395014763, Accuracy: 0.6357421875\n",
      "Traning epoch: 0 / batch:1900, samples: 3891200\n",
      " Train Data Loss: 0.028696516528725624, Accuracy: 0.7158203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022311927750706673, Accuracy: 0.64501953125\n",
      "Traning epoch: 0 / batch:1950, samples: 3993600\n",
      " Train Data Loss: 0.028995763510465622, Accuracy: 0.57666015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021889762952923775, Accuracy: 0.603515625\n",
      "Traning epoch: 0 / batch:2000, samples: 4096000\n",
      " Train Data Loss: 0.031572502106428146, Accuracy: 0.681640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022450324147939682, Accuracy: 0.623046875\n",
      "Traning epoch: 0 / batch:2050, samples: 4198400\n",
      " Train Data Loss: 0.02771906368434429, Accuracy: 0.6064453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022832877933979034, Accuracy: 0.6220703125\n",
      "Traning epoch: 0 / batch:2100, samples: 4300800\n",
      " Train Data Loss: 0.03477351367473602, Accuracy: 0.673828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02260727249085903, Accuracy: 0.623046875\n",
      "Traning epoch: 0 / batch:2150, samples: 4403200\n",
      " Train Data Loss: 0.03237288445234299, Accuracy: 0.55615234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02316584438085556, Accuracy: 0.62548828125\n",
      "Traning epoch: 0 / batch:2200, samples: 4505600\n",
      " Train Data Loss: 0.028641218319535255, Accuracy: 0.6640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02330639958381653, Accuracy: 0.6630859375\n",
      "Traning epoch: 0 / batch:2250, samples: 4608000\n",
      " Train Data Loss: 0.02624862641096115, Accuracy: 0.65966796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022804463282227516, Accuracy: 0.63232421875\n",
      "Traning epoch: 0 / batch:2300, samples: 4710400\n",
      " Train Data Loss: 0.02124139666557312, Accuracy: 0.740234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02224559336900711, Accuracy: 0.65234375\n",
      "Traning epoch: 0 / batch:2350, samples: 4812800\n",
      " Train Data Loss: 0.02677331492304802, Accuracy: 0.62744140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02172829955816269, Accuracy: 0.646484375\n",
      "Traning epoch: 0 / batch:2400, samples: 4915200\n",
      " Train Data Loss: 0.022563526406884193, Accuracy: 0.65380859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022101987153291702, Accuracy: 0.630859375\n",
      "Traning epoch: 0 / batch:2450, samples: 5017600\n",
      " Train Data Loss: 0.025822794064879417, Accuracy: 0.68310546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02247494086623192, Accuracy: 0.65087890625\n",
      "Traning epoch: 0 / batch:2500, samples: 5120000\n",
      " Train Data Loss: 0.027621416375041008, Accuracy: 0.72265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02173750102519989, Accuracy: 0.64599609375\n",
      "Traning epoch: 0 / batch:2550, samples: 5222400\n",
      " Train Data Loss: 0.026969822123646736, Accuracy: 0.6591796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022503718733787537, Accuracy: 0.6357421875\n",
      "Traning epoch: 0 / batch:2600, samples: 5324800\n",
      " Train Data Loss: 0.02885034680366516, Accuracy: 0.70458984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021906491369009018, Accuracy: 0.6572265625\n",
      "Traning epoch: 0 / batch:2650, samples: 5427200\n",
      " Train Data Loss: 0.029056690633296967, Accuracy: 0.6279296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022353922948241234, Accuracy: 0.6376953125\n",
      "Traning epoch: 0 / batch:2700, samples: 5529600\n",
      " Train Data Loss: 0.027898263186216354, Accuracy: 0.6630859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02095816656947136, Accuracy: 0.65771484375\n",
      "Traning epoch: 0 / batch:2750, samples: 5632000\n",
      " Train Data Loss: 0.024808866903185844, Accuracy: 0.638671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021285761147737503, Accuracy: 0.6484375\n",
      "Traning epoch: 0 / batch:2800, samples: 5734400\n",
      " Train Data Loss: 0.022833287715911865, Accuracy: 0.56103515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02065977081656456, Accuracy: 0.654296875\n",
      "Traning epoch: 0 / batch:2850, samples: 5836800\n",
      " Train Data Loss: 0.023118523880839348, Accuracy: 0.712890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020932335406541824, Accuracy: 0.669921875\n",
      "Traning epoch: 1 / batch:2900, samples: 11878400\n",
      " Train Data Loss: 0.02343330718576908, Accuracy: 0.80126953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02062954008579254, Accuracy: 0.68701171875\n",
      "Traning epoch: 1 / batch:2950, samples: 12083200\n",
      " Train Data Loss: 0.02669772319495678, Accuracy: 0.634765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02118688076734543, Accuracy: 0.650390625\n",
      "Traning epoch: 1 / batch:3000, samples: 12288000\n",
      " Train Data Loss: 0.02389519289135933, Accuracy: 0.58349609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020918209105730057, Accuracy: 0.61962890625\n",
      "Traning epoch: 1 / batch:3050, samples: 12492800\n",
      " Train Data Loss: 0.02169104292988777, Accuracy: 0.5947265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02050907537341118, Accuracy: 0.67919921875\n",
      "Traning epoch: 1 / batch:3100, samples: 12697600\n",
      " Train Data Loss: 0.02171291410923004, Accuracy: 0.572265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020844612270593643, Accuracy: 0.63818359375\n",
      "Traning epoch: 1 / batch:3150, samples: 12902400\n",
      " Train Data Loss: 0.022889649495482445, Accuracy: 0.73583984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02149716392159462, Accuracy: 0.7314453125\n",
      "Traning epoch: 1 / batch:3200, samples: 13107200\n",
      " Train Data Loss: 0.02595810405910015, Accuracy: 0.67529296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02048608846962452, Accuracy: 0.7001953125\n",
      "Traning epoch: 1 / batch:3250, samples: 13312000\n",
      " Train Data Loss: 0.025846699252724648, Accuracy: 0.56494140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02166658639907837, Accuracy: 0.6357421875\n",
      "Traning epoch: 1 / batch:3300, samples: 13516800\n",
      " Train Data Loss: 0.0261923186480999, Accuracy: 0.724609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.023339055478572845, Accuracy: 0.67529296875\n",
      "Traning epoch: 1 / batch:3350, samples: 13721600\n",
      " Train Data Loss: 0.02542278543114662, Accuracy: 0.6220703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02269865944981575, Accuracy: 0.63818359375\n",
      "Traning epoch: 1 / batch:3400, samples: 13926400\n",
      " Train Data Loss: 0.024188783019781113, Accuracy: 0.56591796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.022384362295269966, Accuracy: 0.65380859375\n",
      "Traning epoch: 1 / batch:3450, samples: 14131200\n",
      " Train Data Loss: 0.023586278781294823, Accuracy: 0.74267578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021811962127685547, Accuracy: 0.6474609375\n",
      "Traning epoch: 1 / batch:3500, samples: 14336000\n",
      " Train Data Loss: 0.02266201376914978, Accuracy: 0.5791015625\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021167371422052383, Accuracy: 0.67822265625\n",
      "Traning epoch: 1 / batch:3550, samples: 14540800\n",
      " Train Data Loss: 0.020087983459234238, Accuracy: 0.69921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02060190960764885, Accuracy: 0.740234375\n",
      "Traning epoch: 1 / batch:3600, samples: 14745600\n",
      " Train Data Loss: 0.02348044328391552, Accuracy: 0.78369140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02021585777401924, Accuracy: 0.6689453125\n",
      "Traning epoch: 1 / batch:3650, samples: 14950400\n",
      " Train Data Loss: 0.016928989440202713, Accuracy: 0.74365234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021327516064047813, Accuracy: 0.71044921875\n",
      "Traning epoch: 1 / batch:3700, samples: 15155200\n",
      " Train Data Loss: 0.020298223942518234, Accuracy: 0.6611328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021701565012335777, Accuracy: 0.6513671875\n",
      "Traning epoch: 1 / batch:3750, samples: 15360000\n",
      " Train Data Loss: 0.0200120210647583, Accuracy: 0.6796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020955845713615417, Accuracy: 0.7197265625\n",
      "Traning epoch: 1 / batch:3800, samples: 15564800\n",
      " Train Data Loss: 0.02154647558927536, Accuracy: 0.69580078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02045397460460663, Accuracy: 0.66748046875\n",
      "Traning epoch: 1 / batch:3850, samples: 15769600\n",
      " Train Data Loss: 0.015635648742318153, Accuracy: 0.705078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02024136856198311, Accuracy: 0.70556640625\n",
      "Traning epoch: 1 / batch:3900, samples: 15974400\n",
      " Train Data Loss: 0.017736434936523438, Accuracy: 0.7392578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020084768533706665, Accuracy: 0.7490234375\n",
      "Traning epoch: 1 / batch:3950, samples: 16179200\n",
      " Train Data Loss: 0.02107648178935051, Accuracy: 0.67529296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02068827860057354, Accuracy: 0.76953125\n",
      "Traning epoch: 1 / batch:4000, samples: 16384000\n",
      " Train Data Loss: 0.01821328140795231, Accuracy: 0.67431640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020074788480997086, Accuracy: 0.69384765625\n",
      "Traning epoch: 1 / batch:4050, samples: 16588800\n",
      " Train Data Loss: 0.017162252217531204, Accuracy: 0.68603515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01994917169213295, Accuracy: 0.73388671875\n",
      "Traning epoch: 1 / batch:4100, samples: 16793600\n",
      " Train Data Loss: 0.019821297377347946, Accuracy: 0.63623046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020101986825466156, Accuracy: 0.736328125\n",
      "Traning epoch: 1 / batch:4150, samples: 16998400\n",
      " Train Data Loss: 0.020916394889354706, Accuracy: 0.66845703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02000027894973755, Accuracy: 0.7333984375\n",
      "Traning epoch: 1 / batch:4200, samples: 17203200\n",
      " Train Data Loss: 0.02079864777624607, Accuracy: 0.7119140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02005714178085327, Accuracy: 0.7666015625\n",
      "Traning epoch: 1 / batch:4250, samples: 17408000\n",
      " Train Data Loss: 0.022474847733974457, Accuracy: 0.62060546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02056451328098774, Accuracy: 0.697265625\n",
      "Traning epoch: 1 / batch:4300, samples: 17612800\n",
      " Train Data Loss: 0.020864687860012054, Accuracy: 0.763671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020189955830574036, Accuracy: 0.73583984375\n",
      "Traning epoch: 1 / batch:4350, samples: 17817600\n",
      " Train Data Loss: 0.018148239701986313, Accuracy: 0.69482421875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019846569746732712, Accuracy: 0.70556640625\n",
      "Traning epoch: 1 / batch:4400, samples: 18022400\n",
      " Train Data Loss: 0.01996629871428013, Accuracy: 0.63720703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020428938791155815, Accuracy: 0.74169921875\n",
      "Traning epoch: 1 / batch:4450, samples: 18227200\n",
      " Train Data Loss: 0.022292613983154297, Accuracy: 0.73486328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021008361130952835, Accuracy: 0.72607421875\n",
      "Traning epoch: 1 / batch:4500, samples: 18432000\n",
      " Train Data Loss: 0.01872866228222847, Accuracy: 0.78173828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020543411374092102, Accuracy: 0.77197265625\n",
      "Traning epoch: 1 / batch:4550, samples: 18636800\n",
      " Train Data Loss: 0.016089826822280884, Accuracy: 0.625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021202854812145233, Accuracy: 0.7451171875\n",
      "Traning epoch: 1 / batch:4600, samples: 18841600\n",
      " Train Data Loss: 0.01993221417069435, Accuracy: 0.69580078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019958730787038803, Accuracy: 0.80029296875\n",
      "Traning epoch: 1 / batch:4650, samples: 19046400\n",
      " Train Data Loss: 0.020683851093053818, Accuracy: 0.53466796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019729144871234894, Accuracy: 0.75439453125\n",
      "Traning epoch: 1 / batch:4700, samples: 19251200\n",
      " Train Data Loss: 0.01973627135157585, Accuracy: 0.677734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01964792050421238, Accuracy: 0.775390625\n",
      "Traning epoch: 1 / batch:4750, samples: 19456000\n",
      " Train Data Loss: 0.023737724870443344, Accuracy: 0.56005859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019543062895536423, Accuracy: 0.7470703125\n",
      "Traning epoch: 1 / batch:4800, samples: 19660800\n",
      " Train Data Loss: 0.025524822995066643, Accuracy: 0.61669921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01969856023788452, Accuracy: 0.72607421875\n",
      "Traning epoch: 1 / batch:4850, samples: 19865600\n",
      " Train Data Loss: 0.024289067834615707, Accuracy: 0.6279296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01999460905790329, Accuracy: 0.7685546875\n",
      "Traning epoch: 1 / batch:4900, samples: 20070400\n",
      " Train Data Loss: 0.023204218596220016, Accuracy: 0.65478515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019995972514152527, Accuracy: 0.697265625\n",
      "Traning epoch: 1 / batch:4950, samples: 20275200\n",
      " Train Data Loss: 0.02445232681930065, Accuracy: 0.75390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019688239321112633, Accuracy: 0.74755859375\n",
      "Traning epoch: 1 / batch:5000, samples: 20480000\n",
      " Train Data Loss: 0.025927435606718063, Accuracy: 0.72412109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019967064261436462, Accuracy: 0.7099609375\n",
      "Traning epoch: 1 / batch:5050, samples: 20684800\n",
      " Train Data Loss: 0.0220402292907238, Accuracy: 0.64404296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020206626504659653, Accuracy: 0.68603515625\n",
      "Traning epoch: 1 / batch:5100, samples: 20889600\n",
      " Train Data Loss: 0.025253161787986755, Accuracy: 0.6640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019351327791810036, Accuracy: 0.7470703125\n",
      "Traning epoch: 1 / batch:5150, samples: 21094400\n",
      " Train Data Loss: 0.020954914391040802, Accuracy: 0.69287109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0193803608417511, Accuracy: 0.646484375\n",
      "Traning epoch: 1 / batch:5200, samples: 21299200\n",
      " Train Data Loss: 0.016358816996216774, Accuracy: 0.70068359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020106296986341476, Accuracy: 0.67138671875\n",
      "Traning epoch: 1 / batch:5250, samples: 21504000\n",
      " Train Data Loss: 0.0222251545637846, Accuracy: 0.6611328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019832201302051544, Accuracy: 0.64892578125\n",
      "Traning epoch: 1 / batch:5300, samples: 21708800\n",
      " Train Data Loss: 0.019843894988298416, Accuracy: 0.70166015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019751109182834625, Accuracy: 0.66845703125\n",
      "Traning epoch: 1 / batch:5350, samples: 21913600\n",
      " Train Data Loss: 0.02398812025785446, Accuracy: 0.65771484375\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019892549142241478, Accuracy: 0.7001953125\n",
      "Traning epoch: 1 / batch:5400, samples: 22118400\n",
      " Train Data Loss: 0.025778712704777718, Accuracy: 0.689453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019909273833036423, Accuracy: 0.67138671875\n",
      "Traning epoch: 1 / batch:5450, samples: 22323200\n",
      " Train Data Loss: 0.02275455743074417, Accuracy: 0.69921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020041681826114655, Accuracy: 0.66455078125\n",
      "Traning epoch: 1 / batch:5500, samples: 22528000\n",
      " Train Data Loss: 0.024137087166309357, Accuracy: 0.60546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01978295110166073, Accuracy: 0.66943359375\n",
      "Traning epoch: 1 / batch:5550, samples: 22732800\n",
      " Train Data Loss: 0.01953566074371338, Accuracy: 0.66796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019533833488821983, Accuracy: 0.7275390625\n",
      "Traning epoch: 1 / batch:5600, samples: 22937600\n",
      " Train Data Loss: 0.020978178828954697, Accuracy: 0.72314453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020059548318386078, Accuracy: 0.73388671875\n",
      "Traning epoch: 1 / batch:5650, samples: 23142400\n",
      " Train Data Loss: 0.019851725548505783, Accuracy: 0.599609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019325125962495804, Accuracy: 0.69580078125\n",
      "Traning epoch: 1 / batch:5700, samples: 23347200\n",
      " Train Data Loss: 0.020141981542110443, Accuracy: 0.630859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019686799496412277, Accuracy: 0.748046875\n",
      "Traning epoch: 1 / batch:5750, samples: 23552000\n",
      " Train Data Loss: 0.019502636045217514, Accuracy: 0.64013671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02004629373550415, Accuracy: 0.69970703125\n",
      "Traning epoch: 2 / batch:5800, samples: 35635200\n",
      " Train Data Loss: 0.02466326579451561, Accuracy: 0.6552734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02009957656264305, Accuracy: 0.6748046875\n",
      "Traning epoch: 2 / batch:5850, samples: 35942400\n",
      " Train Data Loss: 0.02358999475836754, Accuracy: 0.58837890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020124491304159164, Accuracy: 0.7109375\n",
      "Traning epoch: 2 / batch:5900, samples: 36249600\n",
      " Train Data Loss: 0.024196328595280647, Accuracy: 0.56201171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019700821489095688, Accuracy: 0.61328125\n",
      "Traning epoch: 2 / batch:5950, samples: 36556800\n",
      " Train Data Loss: 0.024733681231737137, Accuracy: 0.6767578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01982663944363594, Accuracy: 0.6845703125\n",
      "Traning epoch: 2 / batch:6000, samples: 36864000\n",
      " Train Data Loss: 0.020693697035312653, Accuracy: 0.595703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020145658403635025, Accuracy: 0.7509765625\n",
      "Traning epoch: 2 / batch:6050, samples: 37171200\n",
      " Train Data Loss: 0.02529323846101761, Accuracy: 0.5439453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02034655399620533, Accuracy: 0.64794921875\n",
      "Traning epoch: 2 / batch:6100, samples: 37478400\n",
      " Train Data Loss: 0.027033984661102295, Accuracy: 0.59375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0211971215903759, Accuracy: 0.62109375\n",
      "Traning epoch: 2 / batch:6150, samples: 37785600\n",
      " Train Data Loss: 0.022455208003520966, Accuracy: 0.7158203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021720897406339645, Accuracy: 0.61181640625\n",
      "Traning epoch: 2 / batch:6200, samples: 38092800\n",
      " Train Data Loss: 0.023622674867510796, Accuracy: 0.4931640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021367624402046204, Accuracy: 0.59814453125\n",
      "Traning epoch: 2 / batch:6250, samples: 38400000\n",
      " Train Data Loss: 0.02219875529408455, Accuracy: 0.60595703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02113475278019905, Accuracy: 0.6123046875\n",
      "Traning epoch: 2 / batch:6300, samples: 38707200\n",
      " Train Data Loss: 0.02106274850666523, Accuracy: 0.65283203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020584290847182274, Accuracy: 0.6435546875\n",
      "Traning epoch: 2 / batch:6350, samples: 39014400\n",
      " Train Data Loss: 0.01642778515815735, Accuracy: 0.73046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0200374536216259, Accuracy: 0.6630859375\n",
      "Traning epoch: 2 / batch:6400, samples: 39321600\n",
      " Train Data Loss: 0.02357790619134903, Accuracy: 0.6083984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019758541136980057, Accuracy: 0.72314453125\n",
      "Traning epoch: 2 / batch:6450, samples: 39628800\n",
      " Train Data Loss: 0.01904979906976223, Accuracy: 0.70166015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019901128485798836, Accuracy: 0.68505859375\n",
      "Traning epoch: 2 / batch:6500, samples: 39936000\n",
      " Train Data Loss: 0.019480295479297638, Accuracy: 0.69970703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019431069493293762, Accuracy: 0.68603515625\n",
      "Traning epoch: 2 / batch:6550, samples: 40243200\n",
      " Train Data Loss: 0.01644815132021904, Accuracy: 0.74609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02046697959303856, Accuracy: 0.638671875\n",
      "Traning epoch: 2 / batch:6600, samples: 40550400\n",
      " Train Data Loss: 0.016572708263993263, Accuracy: 0.71875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.021245939657092094, Accuracy: 0.62548828125\n",
      "Traning epoch: 2 / batch:6650, samples: 40857600\n",
      " Train Data Loss: 0.01732424832880497, Accuracy: 0.65673828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019246535375714302, Accuracy: 0.71044921875\n",
      "Traning epoch: 2 / batch:6700, samples: 41164800\n",
      " Train Data Loss: 0.018734265118837357, Accuracy: 0.74658203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019508950412273407, Accuracy: 0.69775390625\n",
      "Traning epoch: 2 / batch:6750, samples: 41472000\n",
      " Train Data Loss: 0.01630811206996441, Accuracy: 0.69677734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019205976277589798, Accuracy: 0.7119140625\n",
      "Traning epoch: 2 / batch:6800, samples: 41779200\n",
      " Train Data Loss: 0.01608430966734886, Accuracy: 0.7109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019254138693213463, Accuracy: 0.74755859375\n",
      "Traning epoch: 2 / batch:6850, samples: 42086400\n",
      " Train Data Loss: 0.013373905792832375, Accuracy: 0.6611328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019303036853671074, Accuracy: 0.70703125\n",
      "Traning epoch: 2 / batch:6900, samples: 42393600\n",
      " Train Data Loss: 0.017535820603370667, Accuracy: 0.67724609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01917571946978569, Accuracy: 0.662109375\n",
      "Traning epoch: 2 / batch:6950, samples: 42700800\n",
      " Train Data Loss: 0.01807936653494835, Accuracy: 0.77880859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01916143298149109, Accuracy: 0.734375\n",
      "Traning epoch: 2 / batch:7000, samples: 43008000\n",
      " Train Data Loss: 0.018838034942746162, Accuracy: 0.66650390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01939341612160206, Accuracy: 0.6943359375\n",
      "Traning epoch: 2 / batch:7050, samples: 43315200\n",
      " Train Data Loss: 0.015379829332232475, Accuracy: 0.64599609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01940244808793068, Accuracy: 0.64892578125\n",
      "Traning epoch: 2 / batch:7100, samples: 43622400\n",
      " Train Data Loss: 0.018310492858290672, Accuracy: 0.7216796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01927328109741211, Accuracy: 0.72998046875\n",
      "Traning epoch: 2 / batch:7150, samples: 43929600\n",
      " Train Data Loss: 0.019579926505684853, Accuracy: 0.587890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02015029452741146, Accuracy: 0.65185546875\n",
      "Traning epoch: 2 / batch:7200, samples: 44236800\n",
      " Train Data Loss: 0.016977688297629356, Accuracy: 0.53759765625\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019651371985673904, Accuracy: 0.6982421875\n",
      "Traning epoch: 2 / batch:7250, samples: 44544000\n",
      " Train Data Loss: 0.019987137988209724, Accuracy: 0.73828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01961713656783104, Accuracy: 0.7197265625\n",
      "Traning epoch: 2 / batch:7300, samples: 44851200\n",
      " Train Data Loss: 0.01851845346391201, Accuracy: 0.728515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01933671534061432, Accuracy: 0.7119140625\n",
      "Traning epoch: 2 / batch:7350, samples: 45158400\n",
      " Train Data Loss: 0.01918385922908783, Accuracy: 0.71875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01989550143480301, Accuracy: 0.6708984375\n",
      "Traning epoch: 2 / batch:7400, samples: 45465600\n",
      " Train Data Loss: 0.018893329426646233, Accuracy: 0.7138671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019496535882353783, Accuracy: 0.697265625\n",
      "Traning epoch: 2 / batch:7450, samples: 45772800\n",
      " Train Data Loss: 0.012684321962296963, Accuracy: 0.66845703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019714146852493286, Accuracy: 0.64599609375\n",
      "Traning epoch: 2 / batch:7500, samples: 46080000\n",
      " Train Data Loss: 0.017965655773878098, Accuracy: 0.63916015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019177354872226715, Accuracy: 0.72998046875\n",
      "Traning epoch: 2 / batch:7550, samples: 46387200\n",
      " Train Data Loss: 0.019168077036738396, Accuracy: 0.62939453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019102485850453377, Accuracy: 0.7568359375\n",
      "Traning epoch: 2 / batch:7600, samples: 46694400\n",
      " Train Data Loss: 0.01803787425160408, Accuracy: 0.6376953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018816227093338966, Accuracy: 0.63330078125\n",
      "Traning epoch: 2 / batch:7650, samples: 47001600\n",
      " Train Data Loss: 0.02105884626507759, Accuracy: 0.6416015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018794666975736618, Accuracy: 0.6630859375\n",
      "Traning epoch: 2 / batch:7700, samples: 47308800\n",
      " Train Data Loss: 0.02544642612338066, Accuracy: 0.6533203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019199930131435394, Accuracy: 0.712890625\n",
      "Traning epoch: 2 / batch:7750, samples: 47616000\n",
      " Train Data Loss: 0.024459784850478172, Accuracy: 0.669921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019429026171565056, Accuracy: 0.771484375\n",
      "Traning epoch: 2 / batch:7800, samples: 47923200\n",
      " Train Data Loss: 0.02428067848086357, Accuracy: 0.66748046875\n",
      "92583\n",
      "32/32 [==============================] - 1s\n",
      " Validation Loss: 0.019263215363025665, Accuracy: 0.7724609375\n",
      "Traning epoch: 2 / batch:7850, samples: 48230400\n",
      " Train Data Loss: 0.024197053164243698, Accuracy: 0.68212890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018942125141620636, Accuracy: 0.7587890625\n",
      "Traning epoch: 2 / batch:7900, samples: 48537600\n",
      " Train Data Loss: 0.023336779326200485, Accuracy: 0.63134765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018941175192594528, Accuracy: 0.6279296875\n",
      "Traning epoch: 2 / batch:7950, samples: 48844800\n",
      " Train Data Loss: 0.02262984961271286, Accuracy: 0.5302734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01858944445848465, Accuracy: 0.65869140625\n",
      "Traning epoch: 2 / batch:8000, samples: 49152000\n",
      " Train Data Loss: 0.022607631981372833, Accuracy: 0.61572265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01837746426463127, Accuracy: 0.64794921875\n",
      "Traning epoch: 2 / batch:8050, samples: 49459200\n",
      " Train Data Loss: 0.01831604726612568, Accuracy: 0.70166015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018528586253523827, Accuracy: 0.65478515625\n",
      "Traning epoch: 2 / batch:8100, samples: 49766400\n",
      " Train Data Loss: 0.019198454916477203, Accuracy: 0.70458984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018344679847359657, Accuracy: 0.650390625\n",
      "Traning epoch: 2 / batch:8150, samples: 50073600\n",
      " Train Data Loss: 0.01710255816578865, Accuracy: 0.61279296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01878233253955841, Accuracy: 0.6181640625\n",
      "Traning epoch: 2 / batch:8200, samples: 50380800\n",
      " Train Data Loss: 0.02257494255900383, Accuracy: 0.74267578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018537484109401703, Accuracy: 0.685546875\n",
      "Traning epoch: 2 / batch:8250, samples: 50688000\n",
      " Train Data Loss: 0.02008136361837387, Accuracy: 0.7021484375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018676042556762695, Accuracy: 0.6728515625\n",
      "Traning epoch: 2 / batch:8300, samples: 50995200\n",
      " Train Data Loss: 0.021651122719049454, Accuracy: 0.70703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01850661262869835, Accuracy: 0.67138671875\n",
      "Traning epoch: 2 / batch:8350, samples: 51302400\n",
      " Train Data Loss: 0.020062826573848724, Accuracy: 0.69091796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01877499185502529, Accuracy: 0.67822265625\n",
      "Traning epoch: 2 / batch:8400, samples: 51609600\n",
      " Train Data Loss: 0.017309464514255524, Accuracy: 0.65673828125\n",
      "92583\n",
      "32/32 [==============================] - 1s\n",
      " Validation Loss: 0.018544763326644897, Accuracy: 0.642578125\n",
      "Traning epoch: 2 / batch:8450, samples: 51916800\n",
      " Train Data Loss: 0.020525626838207245, Accuracy: 0.66552734375\n",
      "92583\n",
      "32/32 [==============================] - 1s\n",
      " Validation Loss: 0.018200866878032684, Accuracy: 0.7138671875\n",
      "Traning epoch: 2 / batch:8500, samples: 52224000\n",
      " Train Data Loss: 0.0210247952491045, Accuracy: 0.70849609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01828443817794323, Accuracy: 0.7587890625\n",
      "Traning epoch: 2 / batch:8550, samples: 52531200\n",
      " Train Data Loss: 0.0188290998339653, Accuracy: 0.62548828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018282707780599594, Accuracy: 0.7275390625\n",
      "Traning epoch: 2 / batch:8600, samples: 52838400\n",
      " Train Data Loss: 0.01992891915142536, Accuracy: 0.66796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0184281375259161, Accuracy: 0.740234375\n",
      "Traning epoch: 2 / batch:8650, samples: 53145600\n",
      " Train Data Loss: 0.021607201546430588, Accuracy: 0.72705078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018648160621523857, Accuracy: 0.71728515625\n",
      "Traning epoch: 3 / batch:8700, samples: 71270400\n",
      " Train Data Loss: 0.021127350628376007, Accuracy: 0.6953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01878076232969761, Accuracy: 0.6982421875\n",
      "Traning epoch: 3 / batch:8750, samples: 71680000\n",
      " Train Data Loss: 0.019901368767023087, Accuracy: 0.6123046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018604952841997147, Accuracy: 0.7255859375\n",
      "Traning epoch: 3 / batch:8800, samples: 72089600\n",
      " Train Data Loss: 0.019995223730802536, Accuracy: 0.6875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01865069940686226, Accuracy: 0.75341796875\n",
      "Traning epoch: 3 / batch:8850, samples: 72499200\n",
      " Train Data Loss: 0.019877197220921516, Accuracy: 0.64794921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01848893240094185, Accuracy: 0.72412109375\n",
      "Traning epoch: 3 / batch:8900, samples: 72908800\n",
      " Train Data Loss: 0.0217304490506649, Accuracy: 0.58349609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01883566379547119, Accuracy: 0.6796875\n",
      "Traning epoch: 3 / batch:8950, samples: 73318400\n",
      " Train Data Loss: 0.022053293883800507, Accuracy: 0.6337890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018615128472447395, Accuracy: 0.654296875\n",
      "Traning epoch: 3 / batch:9000, samples: 73728000\n",
      " Train Data Loss: 0.02421964891254902, Accuracy: 0.5810546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01961917243897915, Accuracy: 0.62451171875\n",
      "Traning epoch: 3 / batch:9050, samples: 74137600\n",
      " Train Data Loss: 0.020479727536439896, Accuracy: 0.55810546875\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02049875259399414, Accuracy: 0.6240234375\n",
      "Traning epoch: 3 / batch:9100, samples: 74547200\n",
      " Train Data Loss: 0.018101060763001442, Accuracy: 0.599609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02010771632194519, Accuracy: 0.66162109375\n",
      "Traning epoch: 3 / batch:9150, samples: 74956800\n",
      " Train Data Loss: 0.02117731422185898, Accuracy: 0.65869140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020302562043070793, Accuracy: 0.65380859375\n",
      "Traning epoch: 3 / batch:9200, samples: 75366400\n",
      " Train Data Loss: 0.02082229033112526, Accuracy: 0.69580078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019686827436089516, Accuracy: 0.62451171875\n",
      "Traning epoch: 3 / batch:9250, samples: 75776000\n",
      " Train Data Loss: 0.018599899485707283, Accuracy: 0.70068359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019069606438279152, Accuracy: 0.6611328125\n",
      "Traning epoch: 3 / batch:9300, samples: 76185600\n",
      " Train Data Loss: 0.018743470311164856, Accuracy: 0.79248046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019255777820944786, Accuracy: 0.6787109375\n",
      "Traning epoch: 3 / batch:9350, samples: 76595200\n",
      " Train Data Loss: 0.016313748434185982, Accuracy: 0.66552734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019106604158878326, Accuracy: 0.64501953125\n",
      "Traning epoch: 3 / batch:9400, samples: 77004800\n",
      " Train Data Loss: 0.017702236771583557, Accuracy: 0.74755859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018481209874153137, Accuracy: 0.65185546875\n",
      "Traning epoch: 3 / batch:9450, samples: 77414400\n",
      " Train Data Loss: 0.017116062343120575, Accuracy: 0.68310546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018731553107500076, Accuracy: 0.64306640625\n",
      "Traning epoch: 3 / batch:9500, samples: 77824000\n",
      " Train Data Loss: 0.014065418392419815, Accuracy: 0.69775390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019869975745677948, Accuracy: 0.6318359375\n",
      "Traning epoch: 3 / batch:9550, samples: 78233600\n",
      " Train Data Loss: 0.014796406961977482, Accuracy: 0.75341796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018167678266763687, Accuracy: 0.70849609375\n",
      "Traning epoch: 3 / batch:9600, samples: 78643200\n",
      " Train Data Loss: 0.016163475811481476, Accuracy: 0.6796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017626933753490448, Accuracy: 0.67138671875\n",
      "Traning epoch: 3 / batch:9650, samples: 79052800\n",
      " Train Data Loss: 0.016400884836912155, Accuracy: 0.79638671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018229298293590546, Accuracy: 0.7509765625\n",
      "Traning epoch: 3 / batch:9700, samples: 79462400\n",
      " Train Data Loss: 0.01660064235329628, Accuracy: 0.72705078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018348414450883865, Accuracy: 0.734375\n",
      "Traning epoch: 3 / batch:9750, samples: 79872000\n",
      " Train Data Loss: 0.015581450425088406, Accuracy: 0.7998046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018191659823060036, Accuracy: 0.64453125\n",
      "Traning epoch: 3 / batch:9800, samples: 80281600\n",
      " Train Data Loss: 0.01505059190094471, Accuracy: 0.72412109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017986256629228592, Accuracy: 0.66015625\n",
      "Traning epoch: 3 / batch:9850, samples: 80691200\n",
      " Train Data Loss: 0.015375098213553429, Accuracy: 0.7470703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018581030890345573, Accuracy: 0.6748046875\n",
      "Traning epoch: 3 / batch:9900, samples: 81100800\n",
      " Train Data Loss: 0.013836625963449478, Accuracy: 0.70947265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018301790580153465, Accuracy: 0.6279296875\n",
      "Traning epoch: 3 / batch:9950, samples: 81510400\n",
      " Train Data Loss: 0.018630972132086754, Accuracy: 0.66796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018330374732613564, Accuracy: 0.63232421875\n",
      "Traning epoch: 3 / batch:10000, samples: 81920000\n",
      " Train Data Loss: 0.016530273482203484, Accuracy: 0.5537109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018979329615831375, Accuracy: 0.59375\n",
      "Traning epoch: 3 / batch:10050, samples: 82329600\n",
      " Train Data Loss: 0.01766403205692768, Accuracy: 0.71435546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018928537145256996, Accuracy: 0.65283203125\n",
      "Traning epoch: 3 / batch:10100, samples: 82739200\n",
      " Train Data Loss: 0.014991099946200848, Accuracy: 0.75634765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018731243908405304, Accuracy: 0.6826171875\n",
      "Traning epoch: 3 / batch:10150, samples: 83148800\n",
      " Train Data Loss: 0.01600412279367447, Accuracy: 0.56201171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018607039004564285, Accuracy: 0.67578125\n",
      "Traning epoch: 3 / batch:10200, samples: 83558400\n",
      " Train Data Loss: 0.01738756336271763, Accuracy: 0.810546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018926698714494705, Accuracy: 0.76220703125\n",
      "Traning epoch: 3 / batch:10250, samples: 83968000\n",
      " Train Data Loss: 0.016858980059623718, Accuracy: 0.70361328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01912682130932808, Accuracy: 0.71728515625\n",
      "Traning epoch: 3 / batch:10300, samples: 84377600\n",
      " Train Data Loss: 0.014559796079993248, Accuracy: 0.708984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019445329904556274, Accuracy: 0.76318359375\n",
      "Traning epoch: 3 / batch:10350, samples: 84787200\n",
      " Train Data Loss: 0.01627441495656967, Accuracy: 0.7353515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01880771294236183, Accuracy: 0.75732421875\n",
      "Traning epoch: 3 / batch:10400, samples: 85196800\n",
      " Train Data Loss: 0.015888871625065804, Accuracy: 0.53564453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018635300919413567, Accuracy: 0.7421875\n",
      "Traning epoch: 3 / batch:10450, samples: 85606400\n",
      " Train Data Loss: 0.019010715186595917, Accuracy: 0.6025390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01849774271249771, Accuracy: 0.75537109375\n",
      "Traning epoch: 3 / batch:10500, samples: 86016000\n",
      " Train Data Loss: 0.018464576452970505, Accuracy: 0.64208984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018428996205329895, Accuracy: 0.73974609375\n",
      "Traning epoch: 3 / batch:10550, samples: 86425600\n",
      " Train Data Loss: 0.023078221827745438, Accuracy: 0.6396484375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018314173445105553, Accuracy: 0.7421875\n",
      "Traning epoch: 3 / batch:10600, samples: 86835200\n",
      " Train Data Loss: 0.019805381074547768, Accuracy: 0.61962890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018697461113333702, Accuracy: 0.7412109375\n",
      "Traning epoch: 3 / batch:10650, samples: 87244800\n",
      " Train Data Loss: 0.019845757633447647, Accuracy: 0.72900390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018675927072763443, Accuracy: 0.7685546875\n",
      "Traning epoch: 3 / batch:10700, samples: 87654400\n",
      " Train Data Loss: 0.02665184810757637, Accuracy: 0.662109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01816464588046074, Accuracy: 0.7099609375\n",
      "Traning epoch: 3 / batch:10750, samples: 88064000\n",
      " Train Data Loss: 0.025339415296912193, Accuracy: 0.638671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018412213772535324, Accuracy: 0.70556640625\n",
      "Traning epoch: 3 / batch:10800, samples: 88473600\n",
      " Train Data Loss: 0.022309625521302223, Accuracy: 0.60498046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018564626574516296, Accuracy: 0.6279296875\n",
      "Traning epoch: 3 / batch:10850, samples: 88883200\n",
      " Train Data Loss: 0.021578487008810043, Accuracy: 0.6142578125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018367381766438484, Accuracy: 0.62841796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning epoch: 3 / batch:10900, samples: 89292800\n",
      " Train Data Loss: 0.020891934633255005, Accuracy: 0.70947265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01767801120877266, Accuracy: 0.658203125\n",
      "Traning epoch: 3 / batch:10950, samples: 89702400\n",
      " Train Data Loss: 0.016760360449552536, Accuracy: 0.68408203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018079139292240143, Accuracy: 0.65869140625\n",
      "Traning epoch: 3 / batch:11000, samples: 90112000\n",
      " Train Data Loss: 0.017756372690200806, Accuracy: 0.603515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017520856112241745, Accuracy: 0.63330078125\n",
      "Traning epoch: 3 / batch:11050, samples: 90521600\n",
      " Train Data Loss: 0.015408490784466267, Accuracy: 0.7119140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017697522416710854, Accuracy: 0.61767578125\n",
      "Traning epoch: 3 / batch:11100, samples: 90931200\n",
      " Train Data Loss: 0.019664134830236435, Accuracy: 0.66259765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017646461725234985, Accuracy: 0.6640625\n",
      "Traning epoch: 3 / batch:11150, samples: 91340800\n",
      " Train Data Loss: 0.018062468618154526, Accuracy: 0.69921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017831258475780487, Accuracy: 0.64404296875\n",
      "Traning epoch: 3 / batch:11200, samples: 91750400\n",
      " Train Data Loss: 0.020576708018779755, Accuracy: 0.67236328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018211696296930313, Accuracy: 0.64990234375\n",
      "Traning epoch: 3 / batch:11250, samples: 92160000\n",
      " Train Data Loss: 0.016094841063022614, Accuracy: 0.6982421875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017812641337513924, Accuracy: 0.66259765625\n",
      "Traning epoch: 3 / batch:11300, samples: 92569600\n",
      " Train Data Loss: 0.019328713417053223, Accuracy: 0.65234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017775414511561394, Accuracy: 0.65576171875\n",
      "Traning epoch: 3 / batch:11350, samples: 92979200\n",
      " Train Data Loss: 0.018832102417945862, Accuracy: 0.625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01761925220489502, Accuracy: 0.66162109375\n",
      "Traning epoch: 3 / batch:11400, samples: 93388800\n",
      " Train Data Loss: 0.018944822251796722, Accuracy: 0.79345703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017881322652101517, Accuracy: 0.6787109375\n",
      "Traning epoch: 3 / batch:11450, samples: 93798400\n",
      " Train Data Loss: 0.016393546015024185, Accuracy: 0.71728515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017910726368427277, Accuracy: 0.7626953125\n",
      "Traning epoch: 3 / batch:11500, samples: 94208000\n",
      " Train Data Loss: 0.016749463975429535, Accuracy: 0.58349609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017432082444429398, Accuracy: 0.669921875\n",
      "Traning epoch: 4 / batch:11550, samples: 118272000\n",
      " Train Data Loss: 0.021922219544649124, Accuracy: 0.7099609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017622530460357666, Accuracy: 0.6494140625\n",
      "Traning epoch: 4 / batch:11600, samples: 118784000\n",
      " Train Data Loss: 0.0203469879925251, Accuracy: 0.63134765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01753917522728443, Accuracy: 0.68505859375\n",
      "Traning epoch: 4 / batch:11650, samples: 119296000\n",
      " Train Data Loss: 0.019253157079219818, Accuracy: 0.64208984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01780720055103302, Accuracy: 0.71484375\n",
      "Traning epoch: 4 / batch:11700, samples: 119808000\n",
      " Train Data Loss: 0.021290743723511696, Accuracy: 0.59033203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017675217241048813, Accuracy: 0.67724609375\n",
      "Traning epoch: 4 / batch:11750, samples: 120320000\n",
      " Train Data Loss: 0.023044593632221222, Accuracy: 0.568359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01753668487071991, Accuracy: 0.7060546875\n",
      "Traning epoch: 4 / batch:11800, samples: 120832000\n",
      " Train Data Loss: 0.020704910159111023, Accuracy: 0.62255859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018056411296129227, Accuracy: 0.69677734375\n",
      "Traning epoch: 4 / batch:11850, samples: 121344000\n",
      " Train Data Loss: 0.022424787282943726, Accuracy: 0.72705078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018394816666841507, Accuracy: 0.62353515625\n",
      "Traning epoch: 4 / batch:11900, samples: 121856000\n",
      " Train Data Loss: 0.021836504340171814, Accuracy: 0.53955078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019163688644766808, Accuracy: 0.6240234375\n",
      "Traning epoch: 4 / batch:11950, samples: 122368000\n",
      " Train Data Loss: 0.01929209567606449, Accuracy: 0.65673828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.02038053795695305, Accuracy: 0.6474609375\n",
      "Traning epoch: 4 / batch:12000, samples: 122880000\n",
      " Train Data Loss: 0.021669980138540268, Accuracy: 0.60400390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019713303074240685, Accuracy: 0.64892578125\n",
      "Traning epoch: 4 / batch:12050, samples: 123392000\n",
      " Train Data Loss: 0.020761553198099136, Accuracy: 0.736328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01890888623893261, Accuracy: 0.64794921875\n",
      "Traning epoch: 4 / batch:12100, samples: 123904000\n",
      " Train Data Loss: 0.017425667494535446, Accuracy: 0.81689453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018535632640123367, Accuracy: 0.65234375\n",
      "Traning epoch: 4 / batch:12150, samples: 124416000\n",
      " Train Data Loss: 0.016628846526145935, Accuracy: 0.71923828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018606625497341156, Accuracy: 0.6591796875\n",
      "Traning epoch: 4 / batch:12200, samples: 124928000\n",
      " Train Data Loss: 0.014833245426416397, Accuracy: 0.71044921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018360920250415802, Accuracy: 0.640625\n",
      "Traning epoch: 4 / batch:12250, samples: 125440000\n",
      " Train Data Loss: 0.014820914715528488, Accuracy: 0.68212890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017909586429595947, Accuracy: 0.6650390625\n",
      "Traning epoch: 4 / batch:12300, samples: 125952000\n",
      " Train Data Loss: 0.014245998114347458, Accuracy: 0.73193359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017860038205981255, Accuracy: 0.65478515625\n",
      "Traning epoch: 4 / batch:12350, samples: 126464000\n",
      " Train Data Loss: 0.015785744413733482, Accuracy: 0.76318359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018458440899848938, Accuracy: 0.66943359375\n",
      "Traning epoch: 4 / batch:12400, samples: 126976000\n",
      " Train Data Loss: 0.014047352597117424, Accuracy: 0.76025390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01837228611111641, Accuracy: 0.63916015625\n",
      "Traning epoch: 4 / batch:12450, samples: 127488000\n",
      " Train Data Loss: 0.01585305482149124, Accuracy: 0.66259765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017838235944509506, Accuracy: 0.63037109375\n",
      "Traning epoch: 4 / batch:12500, samples: 128000000\n",
      " Train Data Loss: 0.01422174647450447, Accuracy: 0.76123046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017242034897208214, Accuracy: 0.7109375\n",
      "Traning epoch: 4 / batch:12550, samples: 128512000\n",
      " Train Data Loss: 0.013668736442923546, Accuracy: 0.7421875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017246540635824203, Accuracy: 0.72802734375\n",
      "Traning epoch: 4 / batch:12600, samples: 129024000\n",
      " Train Data Loss: 0.015230299904942513, Accuracy: 0.7109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01744556613266468, Accuracy: 0.64892578125\n",
      "Traning epoch: 4 / batch:12650, samples: 129536000\n",
      " Train Data Loss: 0.0132888313382864, Accuracy: 0.7470703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01785648800432682, Accuracy: 0.65625\n",
      "Traning epoch: 4 / batch:12700, samples: 130048000\n",
      " Train Data Loss: 0.013963483273983002, Accuracy: 0.798828125\n",
      "92583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018292700871825218, Accuracy: 0.6552734375\n",
      "Traning epoch: 4 / batch:12750, samples: 130560000\n",
      " Train Data Loss: 0.015075846575200558, Accuracy: 0.68896484375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01844346523284912, Accuracy: 0.62646484375\n",
      "Traning epoch: 4 / batch:12800, samples: 131072000\n",
      " Train Data Loss: 0.016428377479314804, Accuracy: 0.6103515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018120838329195976, Accuracy: 0.650390625\n",
      "Traning epoch: 4 / batch:12850, samples: 131584000\n",
      " Train Data Loss: 0.01666995882987976, Accuracy: 0.63525390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018128249794244766, Accuracy: 0.634765625\n",
      "Traning epoch: 4 / batch:12900, samples: 132096000\n",
      " Train Data Loss: 0.018306324258446693, Accuracy: 0.6279296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018806634470820427, Accuracy: 0.6796875\n",
      "Traning epoch: 4 / batch:12950, samples: 132608000\n",
      " Train Data Loss: 0.01596473529934883, Accuracy: 0.63330078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018320579081773758, Accuracy: 0.62890625\n",
      "Traning epoch: 4 / batch:13000, samples: 133120000\n",
      " Train Data Loss: 0.01663053408265114, Accuracy: 0.6328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018921643495559692, Accuracy: 0.75439453125\n",
      "Traning epoch: 4 / batch:13050, samples: 133632000\n",
      " Train Data Loss: 0.01783135160803795, Accuracy: 0.69384765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019031936302781105, Accuracy: 0.7314453125\n",
      "Traning epoch: 4 / batch:13100, samples: 134144000\n",
      " Train Data Loss: 0.018520936369895935, Accuracy: 0.6748046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01929323375225067, Accuracy: 0.76953125\n",
      "Traning epoch: 4 / batch:13150, samples: 134656000\n",
      " Train Data Loss: 0.019794834777712822, Accuracy: 0.677734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01894475892186165, Accuracy: 0.7587890625\n",
      "Traning epoch: 4 / batch:13200, samples: 135168000\n",
      " Train Data Loss: 0.014843113720417023, Accuracy: 0.6875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01874658651649952, Accuracy: 0.75732421875\n",
      "Traning epoch: 4 / batch:13250, samples: 135680000\n",
      " Train Data Loss: 0.014804976060986519, Accuracy: 0.70654296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018575748428702354, Accuracy: 0.765625\n",
      "Traning epoch: 4 / batch:13300, samples: 136192000\n",
      " Train Data Loss: 0.016935836523771286, Accuracy: 0.7060546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018529009073972702, Accuracy: 0.759765625\n",
      "Traning epoch: 4 / batch:13350, samples: 136704000\n",
      " Train Data Loss: 0.02070983499288559, Accuracy: 0.6103515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018443454056978226, Accuracy: 0.75439453125\n",
      "Traning epoch: 4 / batch:13400, samples: 137216000\n",
      " Train Data Loss: 0.02002618834376335, Accuracy: 0.625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018406854942440987, Accuracy: 0.71337890625\n",
      "Traning epoch: 4 / batch:13450, samples: 137728000\n",
      " Train Data Loss: 0.017515450716018677, Accuracy: 0.6865234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018654512241482735, Accuracy: 0.75390625\n",
      "Traning epoch: 4 / batch:13500, samples: 138240000\n",
      " Train Data Loss: 0.02016116864979267, Accuracy: 0.6943359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018252890557050705, Accuracy: 0.7216796875\n",
      "Traning epoch: 4 / batch:13550, samples: 138752000\n",
      " Train Data Loss: 0.023083172738552094, Accuracy: 0.7158203125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0185910202562809, Accuracy: 0.7314453125\n",
      "Traning epoch: 4 / batch:13600, samples: 139264000\n",
      " Train Data Loss: 0.0230540931224823, Accuracy: 0.59326171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018506653606891632, Accuracy: 0.7138671875\n",
      "Traning epoch: 4 / batch:13650, samples: 139776000\n",
      " Train Data Loss: 0.022180089727044106, Accuracy: 0.65478515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01835671253502369, Accuracy: 0.63330078125\n",
      "Traning epoch: 4 / batch:13700, samples: 140288000\n",
      " Train Data Loss: 0.02515282668173313, Accuracy: 0.62451171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018423359841108322, Accuracy: 0.666015625\n",
      "Traning epoch: 4 / batch:13750, samples: 140800000\n",
      " Train Data Loss: 0.018639925867319107, Accuracy: 0.7099609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018093496561050415, Accuracy: 0.646484375\n",
      "Traning epoch: 4 / batch:13800, samples: 141312000\n",
      " Train Data Loss: 0.01612439565360546, Accuracy: 0.734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017630379647016525, Accuracy: 0.6767578125\n",
      "Traning epoch: 4 / batch:13850, samples: 141824000\n",
      " Train Data Loss: 0.015004729852080345, Accuracy: 0.63671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017985399812459946, Accuracy: 0.6875\n",
      "Traning epoch: 4 / batch:13900, samples: 142336000\n",
      " Train Data Loss: 0.014919621869921684, Accuracy: 0.78955078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017794407904148102, Accuracy: 0.677734375\n",
      "Traning epoch: 4 / batch:13950, samples: 142848000\n",
      " Train Data Loss: 0.014462967403233051, Accuracy: 0.66455078125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017374888062477112, Accuracy: 0.67822265625\n",
      "Traning epoch: 4 / batch:14000, samples: 143360000\n",
      " Train Data Loss: 0.020358005538582802, Accuracy: 0.75634765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017171412706375122, Accuracy: 0.6708984375\n",
      "Traning epoch: 4 / batch:14050, samples: 143872000\n",
      " Train Data Loss: 0.016711879521608353, Accuracy: 0.677734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017659034579992294, Accuracy: 0.62255859375\n",
      "Traning epoch: 4 / batch:14100, samples: 144384000\n",
      " Train Data Loss: 0.018265467137098312, Accuracy: 0.76953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017470382153987885, Accuracy: 0.66259765625\n",
      "Traning epoch: 4 / batch:14150, samples: 144896000\n",
      " Train Data Loss: 0.017884768545627594, Accuracy: 0.609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01710093952715397, Accuracy: 0.65185546875\n",
      "Traning epoch: 4 / batch:14200, samples: 145408000\n",
      " Train Data Loss: 0.01879587210714817, Accuracy: 0.64208984375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017268359661102295, Accuracy: 0.6376953125\n",
      "Traning epoch: 4 / batch:14250, samples: 145920000\n",
      " Train Data Loss: 0.021330811083316803, Accuracy: 0.6484375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017121421173214912, Accuracy: 0.72021484375\n",
      "Traning epoch: 4 / batch:14300, samples: 146432000\n",
      " Train Data Loss: 0.01547155249863863, Accuracy: 0.74365234375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01731709949672222, Accuracy: 0.6640625\n",
      "Traning epoch: 4 / batch:14350, samples: 146944000\n",
      " Train Data Loss: 0.016483809798955917, Accuracy: 0.587890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01748010516166687, Accuracy: 0.74365234375\n",
      "Traning epoch: 4 / batch:14400, samples: 147456000\n",
      " Train Data Loss: 0.01605265960097313, Accuracy: 0.70068359375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017155813053250313, Accuracy: 0.7099609375\n",
      "Traning epoch: 5 / batch:14450, samples: 177561600\n",
      " Train Data Loss: 0.020141620188951492, Accuracy: 0.70361328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.016977068036794662, Accuracy: 0.7197265625\n",
      "Traning epoch: 5 / batch:14500, samples: 178176000\n",
      " Train Data Loss: 0.02181563898921013, Accuracy: 0.6806640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017728030681610107, Accuracy: 0.693359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning epoch: 5 / batch:14550, samples: 178790400\n",
      " Train Data Loss: 0.021779093891382217, Accuracy: 0.65576171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017339710146188736, Accuracy: 0.6591796875\n",
      "Traning epoch: 5 / batch:14600, samples: 179404800\n",
      " Train Data Loss: 0.02100466936826706, Accuracy: 0.61181640625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017189733684062958, Accuracy: 0.68408203125\n",
      "Traning epoch: 5 / batch:14650, samples: 180019200\n",
      " Train Data Loss: 0.0212506465613842, Accuracy: 0.58251953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.016928283497691154, Accuracy: 0.65380859375\n",
      "Traning epoch: 5 / batch:14700, samples: 180633600\n",
      " Train Data Loss: 0.019681448116898537, Accuracy: 0.6904296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017731009051203728, Accuracy: 0.6474609375\n",
      "Traning epoch: 5 / batch:14750, samples: 181248000\n",
      " Train Data Loss: 0.022214962169528008, Accuracy: 0.5986328125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018367623910307884, Accuracy: 0.60888671875\n",
      "Traning epoch: 5 / batch:14800, samples: 181862400\n",
      " Train Data Loss: 0.02266601473093033, Accuracy: 0.6220703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019010528922080994, Accuracy: 0.6259765625\n",
      "Traning epoch: 5 / batch:14850, samples: 182476800\n",
      " Train Data Loss: 0.01932385191321373, Accuracy: 0.6875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.020146246999502182, Accuracy: 0.61962890625\n",
      "Traning epoch: 5 / batch:14900, samples: 183091200\n",
      " Train Data Loss: 0.018038585782051086, Accuracy: 0.7724609375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019537540152668953, Accuracy: 0.65380859375\n",
      "Traning epoch: 5 / batch:14950, samples: 183705600\n",
      " Train Data Loss: 0.021644432097673416, Accuracy: 0.56689453125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018560778349637985, Accuracy: 0.634765625\n",
      "Traning epoch: 5 / batch:15000, samples: 184320000\n",
      " Train Data Loss: 0.019127195701003075, Accuracy: 0.72998046875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01815386861562729, Accuracy: 0.6943359375\n",
      "Traning epoch: 5 / batch:15050, samples: 184934400\n",
      " Train Data Loss: 0.017837848514318466, Accuracy: 0.61669921875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01879248395562172, Accuracy: 0.66796875\n",
      "Traning epoch: 5 / batch:15100, samples: 185548800\n",
      " Train Data Loss: 0.01710091345012188, Accuracy: 0.63134765625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018238980323076248, Accuracy: 0.64501953125\n",
      "Traning epoch: 5 / batch:15150, samples: 186163200\n",
      " Train Data Loss: 0.018200725317001343, Accuracy: 0.71728515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017641905695199966, Accuracy: 0.66552734375\n",
      "Traning epoch: 5 / batch:15200, samples: 186777600\n",
      " Train Data Loss: 0.011038306169211864, Accuracy: 0.65966796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017415180802345276, Accuracy: 0.6494140625\n",
      "Traning epoch: 5 / batch:15250, samples: 187392000\n",
      " Train Data Loss: 0.014215041883289814, Accuracy: 0.70703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018063034862279892, Accuracy: 0.6376953125\n",
      "Traning epoch: 5 / batch:15300, samples: 188006400\n",
      " Train Data Loss: 0.016689416021108627, Accuracy: 0.7529296875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01739029586315155, Accuracy: 0.66650390625\n",
      "Traning epoch: 5 / batch:15350, samples: 188620800\n",
      " Train Data Loss: 0.014103056862950325, Accuracy: 0.68994140625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01716414839029312, Accuracy: 0.6728515625\n",
      "Traning epoch: 5 / batch:15400, samples: 189235200\n",
      " Train Data Loss: 0.014756157994270325, Accuracy: 0.77001953125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.0167912058532238, Accuracy: 0.65966796875\n",
      "Traning epoch: 5 / batch:15450, samples: 189849600\n",
      " Train Data Loss: 0.014482839033007622, Accuracy: 0.6787109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017022686079144478, Accuracy: 0.64111328125\n",
      "Traning epoch: 5 / batch:15500, samples: 190464000\n",
      " Train Data Loss: 0.013477263040840626, Accuracy: 0.59912109375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017645223066210747, Accuracy: 0.59765625\n",
      "Traning epoch: 5 / batch:15550, samples: 191078400\n",
      " Train Data Loss: 0.01067667081952095, Accuracy: 0.79052734375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017994575202465057, Accuracy: 0.6669921875\n",
      "Traning epoch: 5 / batch:15600, samples: 191692800\n",
      " Train Data Loss: 0.012972540222108364, Accuracy: 0.77685546875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01846507005393505, Accuracy: 0.64306640625\n",
      "Traning epoch: 5 / batch:15650, samples: 192307200\n",
      " Train Data Loss: 0.013630752451717854, Accuracy: 0.63671875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018493713811039925, Accuracy: 0.65234375\n",
      "Traning epoch: 5 / batch:15700, samples: 192921600\n",
      " Train Data Loss: 0.015045720152556896, Accuracy: 0.63525390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.017560353502631187, Accuracy: 0.619140625\n",
      "Traning epoch: 5 / batch:15750, samples: 193536000\n",
      " Train Data Loss: 0.01506025344133377, Accuracy: 0.7587890625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018054254353046417, Accuracy: 0.64892578125\n",
      "Traning epoch: 5 / batch:15800, samples: 194150400\n",
      " Train Data Loss: 0.013089900836348534, Accuracy: 0.720703125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018302422016859055, Accuracy: 0.6865234375\n",
      "Traning epoch: 5 / batch:15850, samples: 194764800\n",
      " Train Data Loss: 0.014944246038794518, Accuracy: 0.75341796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018399987369775772, Accuracy: 0.71435546875\n",
      "Traning epoch: 5 / batch:15900, samples: 195379200\n",
      " Train Data Loss: 0.017813995480537415, Accuracy: 0.64697265625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018863245844841003, Accuracy: 0.66357421875\n",
      "Traning epoch: 5 / batch:15950, samples: 195993600\n",
      " Train Data Loss: 0.016484569758176804, Accuracy: 0.6025390625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01918930932879448, Accuracy: 0.658203125\n",
      "Traning epoch: 5 / batch:16000, samples: 196608000\n",
      " Train Data Loss: 0.018136514350771904, Accuracy: 0.728515625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.019438069313764572, Accuracy: 0.68212890625\n",
      "Traning epoch: 5 / batch:16050, samples: 197222400\n",
      " Train Data Loss: 0.012510228902101517, Accuracy: 0.74951171875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018937595188617706, Accuracy: 0.71630859375\n",
      "Traning epoch: 5 / batch:16100, samples: 197836800\n",
      " Train Data Loss: 0.016364913433790207, Accuracy: 0.71630859375\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01834079995751381, Accuracy: 0.71484375\n",
      "Traning epoch: 5 / batch:16150, samples: 198451200\n",
      " Train Data Loss: 0.01843332126736641, Accuracy: 0.6923828125\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.01863488182425499, Accuracy: 0.75\n",
      "Traning epoch: 5 / batch:16200, samples: 199065600\n",
      " Train Data Loss: 0.015991441905498505, Accuracy: 0.66796875\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018252724781632423, Accuracy: 0.7646484375\n",
      "Traning epoch: 5 / batch:16250, samples: 199680000\n",
      " Train Data Loss: 0.018039420247077942, Accuracy: 0.67041015625\n",
      "92583\n",
      "32/32 [==============================] - 0s\n",
      " Validation Loss: 0.018086664378643036, Accuracy: 0.76416015625\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras import models as tfm\n",
    "from tensorflow.contrib.keras import layers as tfl\n",
    "from tensorflow.contrib.keras import optimizers\n",
    "from tensorflow.contrib.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.contrib.keras import wrappers as tfw\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.keras import backend as K\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "n_vocab = len(dictionary)+1\n",
    "#n_embed_size = 20\n",
    "p_dropout = 0.3\n",
    "\n",
    "SEQLEN = 64\n",
    "BATCHSIZE = 32\n",
    "INTERNALSIZE = 256\n",
    "NB_EPOCHS = 10\n",
    "\n",
    "def cos_distance(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = raw_data\n",
    "    data_len = data.shape[0]\n",
    "    print(data_len)\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len, :], [batch_size, nb_batches * sequence_size, MAX_VOICES*2])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1, :], [batch_size, nb_batches * sequence_size, MAX_VOICES*2])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "            \n",
    "def rnn_validata_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = raw_data\n",
    "    data_len = data.shape[0]\n",
    "    print(data_len)\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len, :], [batch_size, nb_batches * sequence_size, MAX_VOICES*2])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1, :], [batch_size, nb_batches * sequence_size, MAX_VOICES*2])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch\n",
    "\n",
    "def create_model():\n",
    "# create model\n",
    "    model = tfm.Sequential()\n",
    "    model.add(tfl.LSTM(INTERNALSIZE, return_sequences=True,\n",
    "               input_shape=(SEQLEN, MAX_VOICES*2), dropout = p_dropout, recurrent_dropout = p_dropout)) \n",
    "    #model.add(tfl.Dropout(p_dropout))\n",
    "    model.add(tfl.LSTM(INTERNALSIZE, return_sequences=True,\n",
    "             input_shape=(SEQLEN, MAX_VOICES*2), dropout = p_dropout, recurrent_dropout = p_dropout))  # returns a sequence of vectors of dimension 32\n",
    "    #model.add(tfl.Dropout(p_dropout))\n",
    "    model.add(tfl.LSTM(INTERNALSIZE, return_sequences=True,\n",
    "             input_shape=(SEQLEN, MAX_VOICES*2), dropout = p_dropout, recurrent_dropout = p_dropout))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(tfl.TimeDistributed(tfl.Dense(MAX_VOICES*2, activation='relu')))\n",
    "    model.compile(loss='mse',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())\n",
    "\n",
    "step = 0\n",
    "for x, y_, epoch in rnn_minibatch_sequencer(codevector, BATCHSIZE, SEQLEN, nb_epochs=NB_EPOCHS):\n",
    "    history = model.train_on_batch(x, y_)\n",
    "    if step % 50 == 0 and len(valitext) > 0:\n",
    "        print(\"Traning epoch: {} / batch:{}, samples: {}\".format(epoch, step, (epoch+1)*step*BATCHSIZE*SEQLEN))\n",
    "        print(\" Train Data Loss: {}, Accuracy: {}\".format(history[0], history[1]))\n",
    "        vali_x, vali_y, _ = next(rnn_validata_sequencer(valivector, BATCHSIZE, SEQLEN, nb_epochs=NB_EPOCHS)) \n",
    "        loss, accuracy = model.evaluate(vali_x, vali_y, verbose =1)\n",
    "        print(\" Validation Loss: {}, Accuracy: {}\".format(loss, accuracy))       \n",
    "    step += 1\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.96982968, 0.04296875]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 20 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  # embeddings = tf.Variable(tf.random_uniform([n_vocab, embedding_size], -1.0, 1.0))\n",
    "  embeddings = tf.Variable(final_embeddings) #use this to continue learning from the saved checkpoint\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outfile = Path(PATH + \"\\\\final_embeddings_3.npy\")\n",
    "final_embeddings = np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Nearest to    d:    b,    c,    f, q J\\,  P `, m] L,\n",
      "Nearest to f   : h   , d   , i   , g   ,   R\\, gLa[,\n",
      "Nearest to    ]: QUVS, ]_GV,  fb[, `[ I,    _, MT] ,\n",
      "Nearest to  L  :  P  ,  S  , ]ebS,    =,  Q  , ZL[O,\n",
      "Nearest to    T:  NX , ` M[, JX L,   V ,   S ,  Qkk,\n",
      "Nearest to  Q  :  S  ,  P  , ]VYJ,  O  , ZL[O,  XOQ,\n",
      "Nearest to [   : Z   , V   , U   , Kg]H, S_D , fb l,\n",
      "Nearest to  X  :  Z  ,  V  ,  U  , dd h, lhY ,  Yi[,\n",
      "Nearest to   X :   Z ,   V ,   U , ni` , ^Hl , MXi],\n",
      "Nearest to    P:    L,    Q,    N,    I,    O,    J,\n",
      "Nearest to    a:    _,    c, JV^ , ZmN ,    d, dD \\,\n",
      "Nearest to  S  :  Q  ,  P  ,  O  ,  L  ,  XOQ, dJ[T,\n",
      "Nearest to S   : U   , V   , s da, X   , Q   ,  _Kr,\n",
      "Nearest to UNK: W_Wi, MiVb, W[_i,  j n,  `pP, gG _,\n",
      "Nearest to   ] :   [ , t  _,  _Oh,   Z , B  ], ^i d,\n",
      "Nearest to  E  :  I  ,  J  , a  p, aMQg, eid ,  H J,\n",
      "Average loss at step 1000: 8.236681\n",
      "Nearest to    d:    b,    c,    f,  P `,    a,    h,\n",
      "Nearest to f   : h   , d   , W i , g   ,   R\\, m Q ,\n",
      "Nearest to    ]: QUVS,    _,  fb[, ]_GV, `[ I, a  o,\n",
      "Nearest to  L  :  P  ,     ,  S  ,  G  ,  I  ,  N  ,\n",
      "Nearest to    T:   X ,   V ,   S ,  NX ,   J ,   U ,\n",
      "Nearest to  Q  :  S  ,  P  ,  L  ,  N  ,  E  ,  J  ,\n",
      "Nearest to [   : Z   , Kg]H, X   , V   , U   , S_D ,\n",
      "Nearest to  X  :  V  ,  U  ,  Z  ,  O  ,   T ,  W  ,\n",
      "Nearest to   X :   V , ni` ,   U ,   Z ,   S ,   Q ,\n",
      "Nearest to    P:    L,    N,    Q,    J,    U,    I,\n",
      "Nearest to    a:    _,    c, JV^ , ZmN ,    d, E_  ,\n",
      "Nearest to  S  :  P  ,  Q  ,  L  ,  U  ,  N  ,  J  ,\n",
      "Nearest to S   : V   , Q   , U   , P   ,  T  , L   ,\n",
      "Nearest to UNK: W_Wi, MiVb, W[_i,  j n,  `pP, gG _,\n",
      "Nearest to   ] :   [ , t  _,   aQ,   Z ,  _Oh,  eTp,\n",
      "Nearest to  E  :  L  ,  J  ,  I  ,  S  ,  Q  ,  P  ,\n",
      "Average loss at step 2000: 3.783873\n",
      "Nearest to    d:    b,    c,    f,  P `,    a,    h,\n",
      "Nearest to f   : h   , d   , W i , g   ,   R\\, m Q ,\n",
      "Nearest to    ]: QUVS,    _,  fb[, ]_GV, `[ I, a  o,\n",
      "Nearest to  L  :  I  ,     ,  G  ,  N  ,  K  ,  P  ,\n",
      "Nearest to    T:   X ,   V ,   J ,   U ,   O ,   S ,\n",
      "Nearest to  Q  :  S  ,  P  ,  J  ,  E  ,  U  ,  I  ,\n",
      "Nearest to [   : Z   , Kg]H, S_D , fb l, pLX ,  NN_,\n",
      "Nearest to  X  :  E  ,  U  ,  V  ,  J  ,  I  ,  S  ,\n",
      "Nearest to   X :   V ,   U ,   S ,   Q ,   L ,   N ,\n",
      "Nearest to    P:    N,    Q,    L,    S,    U,    X,\n",
      "Nearest to    a:    _,    c, JV^ , ZmN ,    d, E_  ,\n",
      "Nearest to  S  :  P  ,  U  ,  J  ,  Q  ,  V  ,  O  ,\n",
      "Nearest to S   : V   , Q   , P   , U   , L   ,  T  ,\n",
      "Nearest to UNK: W_Wi, MiVb, W[_i,  j n,  `pP, gG _,\n",
      "Nearest to   ] :   [ , t  _,   aQ,  _Oh,  eTp, ^i d,\n",
      "Nearest to  E  :  X  ,  I  ,  J  ,  Q  ,  D  ,  U  ,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "refresh_freq = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % refresh_freq == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / refresh_freq\n",
    "            # The average loss is an estimate of the loss over the last refresh_freq batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        sim = similarity.eval()\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 6 # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "            log = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log = '%s %s,' % (log, close_word)\n",
    "            print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codevector = np.zeros((len(codetext), MAX_VOICES*2))\n",
    "for i in range(len(codetext)):\n",
    "    bar = reverse_dictionary[codetext[i]]\n",
    "    for j in range(MAX_VOICES):\n",
    "        codevector[i][2*j] = (ord(bar[j])-32)/127\n",
    "        if bar[j] == \" \":\n",
    "            codevector[i][2*j+1] = 0\n",
    "        else:\n",
    "            codevector[i][2*j+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valivector = np.zeros((len(valitext), MAX_VOICES*2))\n",
    "for i in range(len(valitext)):\n",
    "    bar = reverse_dictionary[valitext[i]]\n",
    "    for j in range(MAX_VOICES):\n",
    "        valivector[i][2*j] = (ord(bar[j])-32)/127\n",
    "        if bar[j] == \" \":\n",
    "            valivector[i][2*j+1] = 0\n",
    "        else:\n",
    "            valivector[i][2*j+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57480315,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.59055118,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.60629921,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.34645669, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.29133858, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.07874016,  1.        ,  0.07874016, ...,  1.        ,\n",
       "         0.07874016,  1.        ]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codevector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to    d:    b,    c,    f,  P `,    a,    h, Q Xb, HT V, m] L, aUgQ,  [QE,    i,\n",
      "Nearest to f   : h   , d   , W i , g   ,   R\\, m Q , b   , n^T , i   , [Y  , iU  , gLa[,\n",
      "Nearest to    ]: QUVS,    _,  fb[, ]_GV, `[ I, a  o, []LB, MT] , F[b^, iQZa, QVMS, X Ob,\n",
      "Nearest to  L  :  I  ,     ,  G  ,  N  ,  K  ,  P  ,   M ,   K ,    @,  S  ,  B  ,  J  ,\n",
      "Nearest to    T:   X ,   V ,   J ,   U ,   O ,   S ,   Q ,   G ,   L ,   P ,   I ,   Z ,\n",
      "Nearest to  Q  :  S  ,  P  ,  J  ,  E  ,  U  ,  I  ,  N  ,  L  ,  D  ,  V  ,  O  ,  X  ,\n",
      "Nearest to [   : Z   , Kg]H, S_D , fb l, pLX ,  NN_, c\\l , mW d, sZ p, X   , oi T, h XX,\n",
      "Nearest to  X  :  E  ,  U  ,  V  ,  J  ,  I  ,  S  ,  Q  ,  Z  ,  P  ,   B ,  G  ,  L  ,\n",
      "Nearest to   X :   V ,   U ,   S ,   Q ,   L ,   N ,   G ,   P ,   I ,   J , ********,   Z ,\n",
      "Nearest to    P:    N,    Q,    L,    S,    U,    X,    J,    V,    R,    I,    O,    G,\n",
      "Nearest to    a:    _,    c, JV^ , ZmN ,    d, E_  , dD \\, c GS, fm X,  kXL, Z dK,  dYO,\n",
      "Nearest to  S  :  P  ,  U  ,  J  ,  Q  ,  V  ,  O  ,  N  ,  G  ,  L  ,  I  ,  E  ,  R  ,\n",
      "Nearest to S   : V   , Q   , P   , U   , L   ,  T  ,  M  , I   , ********, N   ,     ,   E ,\n",
      "Nearest to UNK: W_Wi, MiVb, W[_i,  j n,  `pP, gG _,  aaI, [[EV, l d`, iX H, lbJ , Vfg ,\n",
      "Nearest to   ] :   [ , t  _,   aQ,  _Oh,  eTp, ^i d, B  ],   ` ,  PZ ,   Y ,  Ubk,  U \\,\n",
      "Nearest to  E  :  X  ,  I  ,  J  ,  Q  ,  D  ,  U  ,  V  ,  L  ,  S  ,  P  ,  G  ,     ,\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "    top_k = 12 # number of nearest neighbors\n",
    "    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "    log = 'Nearest to %s:' % valid_word\n",
    "    for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40275112, -0.04863255,  0.30368793,  0.06228257, -0.28215745,\n",
       "       -0.04976769,  0.18830134, -0.0575086 ,  0.16519442,  0.07572614,\n",
       "       -0.35413435, -0.136016  ,  0.05114105,  0.23464742, -0.01558306,\n",
       "        0.21568705,  0.38817203, -0.12697509, -0.40035945, -0.1000249 ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outfile = Path(PATH + \"/final_embeddings_4.npy\")\n",
    "np.save(outfile, final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\krzysztof.pieranski\\\\Documents\\\\ml\\\\deepbach'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_vector = np.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's substitute integer codes for notes with corresponding embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 20 # Dimension of the embedding vector.\n",
    "n_neurons = 256\n",
    "n_layers = \n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, [batch_size, SEQLEN, 1])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, SEQLEN, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "\n",
    "  # Constant. We use pre-trained embeddings\n",
    "  embeddings = tf.Constant(final_embeddings) #use this to continue learning from the saved checkpoint\n",
    "\n",
    "  # Model.\n",
    "  \n",
    "  # Look up embeddings for inputs.\n",
    "  input_embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  \n",
    "  layers = [tf.contrib.rnn.LSTMCell(n_neurons, activation = tf.nn.relu) for layer in range(n_layers)]\n",
    "  multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "  outputs, states = tf.nn.dynamic_rnn(multi_layer_cell)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdc\n",
      "acbd\n",
      "acdb\n",
      "adbc\n",
      "adcb\n",
      "bacd\n",
      "badc\n",
      "bcad\n",
      "bcda\n",
      "bdac\n",
      "bdca\n",
      "cabd\n",
      "cadb\n",
      "cbad\n",
      "cbda\n",
      "cdab\n",
      "cdba\n",
      "dabc\n",
      "dacb\n",
      "dbac\n",
      "dbca\n",
      "dcab\n",
      "dcba\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(b)):\n",
    "    print(permutate(\"abcd\", b[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2, 3),\n",
       " (0, 1, 3, 2),\n",
       " (0, 2, 1, 3),\n",
       " (0, 2, 3, 1),\n",
       " (0, 3, 1, 2),\n",
       " (0, 3, 2, 1),\n",
       " (1, 0, 2, 3),\n",
       " (1, 0, 3, 2),\n",
       " (1, 2, 0, 3),\n",
       " (1, 2, 3, 0),\n",
       " (1, 3, 0, 2),\n",
       " (1, 3, 2, 0),\n",
       " (2, 0, 1, 3),\n",
       " (2, 0, 3, 1),\n",
       " (2, 1, 0, 3),\n",
       " (2, 1, 3, 0),\n",
       " (2, 3, 0, 1),\n",
       " (2, 3, 1, 0),\n",
       " (3, 0, 1, 2),\n",
       " (3, 0, 2, 1),\n",
       " (3, 1, 0, 2),\n",
       " (3, 1, 2, 0),\n",
       " (3, 2, 0, 1),\n",
       " (3, 2, 1, 0)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'********'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.48772229515735"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aabb    ', '    hhii', '  kkll  ']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_size = 64\n",
    "data = np.array(codetext)\n",
    "data_len = data.shape[0]\n",
    "# using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,      1,      1, ...,    776,     96,    153],\n",
       "       [    32,     92,    595, ...,     50,      6,      1],\n",
       "       [     1,      1,      1, ...,    154,    154,     97],\n",
       "       ..., \n",
       "       [     1,      1,      4, ...,      9,     36,     13],\n",
       "       [     9,    145,    145, ...,    806,     76,     76],\n",
       "       [     4,      4,     24, ..., 112439,   1087, 154318]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 184768)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5912576"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*184768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
